{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T13:57:38.116276Z","iopub.execute_input":"2021-08-02T13:57:38.116691Z","iopub.status.idle":"2021-08-02T13:57:38.855391Z","shell.execute_reply.started":"2021-08-02T13:57:38.116611Z","shell.execute_reply":"2021-08-02T13:57:38.85459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\nfrom typing import Dict, Any, Union\n\nfrom pathlib import Path\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\n\nimport torch.utils.data as D\nfrom torch.utils.data.dataset import Dataset, IterableDataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import PreTrainedModel\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:39.416164Z","iopub.execute_input":"2021-08-02T13:57:39.416545Z","iopub.status.idle":"2021-08-02T13:57:42.156896Z","shell.execute_reply.started":"2021-08-02T13:57:39.416512Z","shell.execute_reply":"2021-08-02T13:57:42.155884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Folders and Dataframes","metadata":{}},{"cell_type":"code","source":"DATA_PATH = Path('/kaggle/input/commonlitreadabilityprize/')\nassert DATA_PATH.exists()\nMODELS_PATH_BART = Path('/kaggle/input/commonlitbartlargelight/best_models')\nassert MODELS_PATH_BART.exists()\nMODELS_PATH_ROBERTA_LARGE = Path('/kaggle/input/commonlit-roberta-large-light/best_models')\nassert MODELS_PATH_ROBERTA_LARGE.exists()\nMODELS_PATH_ROBERTA_BALANCED = Path('/kaggle/input/commonlit-roberta-base-light-balanced/best_models')\nassert MODELS_PATH_ROBERTA_BALANCED.exists()\nMODELS_PATH_ROBERTA_LIGHT_2 = Path('/kaggle/input/commonlit-roberta-large-light-2/best_models')\nassert MODELS_PATH_ROBERTA_LIGHT_2.exists()\nMODELS_PATH_ROBERTA_BASE_LIGHT = Path('/kaggle/input/commonlit-roberta-base-light/best_models')\nassert MODELS_PATH_ROBERTA_BASE_LIGHT.exists()\nMODELS_PATH_DEBERTA_LARGE = Path('/kaggle/input/commonlit-deberta-large/best_models')\nassert MODELS_PATH_DEBERTA_LARGE.exists()\nMODELS_PATH_T5_LARGE = Path('/kaggle/input/commonlit-t5-large/best_models')\nassert MODELS_PATH_T5_LARGE.exists()\nMODELS_PATH_ELECTRA_LARGE = Path('/kaggle/input/commonlit-electra-large-discriminator/best_models')\nassert MODELS_PATH_T5_LARGE.exists()\nMODELS_PATH_DEBERTA_UNIFORM = Path('/kaggle/input/commonlit-deberta-large-uniform/best_models')\nassert MODELS_PATH_DEBERTA_UNIFORM.exists()\nMODELS_PATH_DEBERTA_XLARGE = Path('/kaggle/input/commonlit-deberta-xlarge/best_models')\nassert MODELS_PATH_DEBERTA_XLARGE.exists()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:42.15837Z","iopub.execute_input":"2021-08-02T13:57:42.158681Z","iopub.status.idle":"2021-08-02T13:57:42.167445Z","shell.execute_reply.started":"2021-08-02T13:57:42.158646Z","shell.execute_reply":"2021-08-02T13:57:42.164975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(DATA_PATH/'train.csv')\ntest_df = pd.read_csv(DATA_PATH/'test.csv')\nsample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:42.169559Z","iopub.execute_input":"2021-08-02T13:57:42.170139Z","iopub.status.idle":"2021-08-02T13:57:42.287445Z","shell.execute_reply.started":"2021-08-02T13:57:42.170063Z","shell.execute_reply":"2021-08-02T13:57:42.286477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_unnecessary(df):\n    df.drop(df[df['target'] == 0].index, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    \nremove_unnecessary(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:42.28963Z","iopub.execute_input":"2021-08-02T13:57:42.289883Z","iopub.status.idle":"2021-08-02T13:57:42.313708Z","shell.execute_reply.started":"2021-08-02T13:57:42.289856Z","shell.execute_reply":"2021-08-02T13:57:42.312853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"def assert_path(path):\n    assert Path(path).exists(), f\"{path} is missing\"","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:43.770838Z","iopub.execute_input":"2021-08-02T13:57:43.771237Z","iopub.status.idle":"2021-08-02T13:57:43.777896Z","shell.execute_reply.started":"2021-08-02T13:57:43.771202Z","shell.execute_reply":"2021-08-02T13:57:43.77679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG():\n    model_name = 'multi-models'\n    batch_size = 48\n    max_len = 256\n    save_dir = f'trained/{model_name}'\n    num_workers = 2\n    epochs = 20\n    pretrained_bart_lm_model = '/kaggle/input/commonlitbartlargelight/lm'\n    pretrained_roberta_large_lm_model = '/kaggle/input/commonlit-roberta-large-light/lm'\n    pretrained_roberta_base_light_lm_model = '/kaggle/input/commonlit-roberta-base-light/lm'\n    pretrained_roberta_light_balanced_lm_model = '/kaggle/input/commonlit-roberta-base-light-balanced/lm'\n    pretrained_roberta_light_2_lm_model = '/kaggle/input/commonlit-roberta-large-light-2/lm'\n    pretrained_deberta_large_lm_model = '/kaggle/input/commonlit-deberta-large/lm'\n    pretrained_t5_large_lm_model = '/kaggle/input/commonlit-t5-large/lm'\n    pretrained_electra_large_lm_model = str(MODELS_PATH_ELECTRA_LARGE.parent/'lm')\n    pretrained_deberta_uniform_model = str(MODELS_PATH_DEBERTA_UNIFORM.parent/'lm')\n    pretrained_deberta_xlarge_model = str(MODELS_PATH_DEBERTA_XLARGE.parent/'lm')\n    assert_path(pretrained_bart_lm_model)\n    assert_path(pretrained_roberta_large_lm_model)\n    assert_path(pretrained_roberta_base_light_lm_model)\n    assert_path(pretrained_roberta_light_balanced_lm_model)\n    assert_path(pretrained_roberta_light_2_lm_model)\n    assert_path(pretrained_deberta_large_lm_model)\n    assert_path(pretrained_t5_large_lm_model)\n    assert_path(pretrained_electra_large_lm_model)\n    assert_path(pretrained_deberta_uniform_model)\n    assert_path(pretrained_deberta_xlarge_model)\n    n_folds = 5\n    model_offset = 0\n    model_limit = 5\n    svm_c = 10\n    svm_kernels = ['rbf']\n    \ncfg = CONFIG()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:44.400152Z","iopub.execute_input":"2021-08-02T13:57:44.400473Z","iopub.status.idle":"2021-08-02T13:57:44.408575Z","shell.execute_reply.started":"2021-08-02T13:57:44.400443Z","shell.execute_reply":"2021-08-02T13:57:44.40769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Existing Models","metadata":{}},{"cell_type":"code","source":"models_map = {\n    'deberta_xlarge' : {'model': 'CommonLitDebertaXLargeModel', 'tokenizer_path': MODELS_PATH_DEBERTA_XLARGE, 'weight': 0.15},\n    'deberta_uniform' : {'model': 'CommonLitLightModel', 'tokenizer_path': MODELS_PATH_DEBERTA_UNIFORM, 'weight': 0.16},\n    'electra_large' : {'model': 'CommonLitElectraModel', 'tokenizer_path': MODELS_PATH_ELECTRA_LARGE, 'weight': 0.13},\n#     't5_large' : {'model': 'CommonLitT5Model', 'tokenizer_path': MODELS_PATH_T5_LARGE, 'weight': 0.11},\n    'roberta_deberta_large': {'model': 'CommonLitLightModel', 'tokenizer_path': MODELS_PATH_DEBERTA_LARGE, 'weight': 0.18},\n    'roberta_base_light' : {'model': 'CommonLitLightModel', 'tokenizer_path': MODELS_PATH_ROBERTA_BASE_LIGHT, 'weight': 0.11},\n    'roberta_light_balanced' : {'model': 'CommonLitLightModel', 'tokenizer_path': MODELS_PATH_ROBERTA_BALANCED, 'weight': 0.11},\n#     'bart' : {'model': 'CommonLitBartModel', 'tokenizer_path': MODELS_PATH_BART, 'weight': 0.11},\n#     'roberta_light_2': {'model': 'CommonLitLightModel', 'tokenizer_path': MODELS_PATH_ROBERTA_LIGHT_2, 'weight': 0.11},\n    'roberta_large' : {'model': 'CommonLitRobertaLargeModel', 'tokenizer_path': MODELS_PATH_ROBERTA_LARGE, 'weight': 0.16}\n}   ","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:45.976036Z","iopub.execute_input":"2021-08-02T13:57:45.976407Z","iopub.status.idle":"2021-08-02T13:57:45.982259Z","shell.execute_reply.started":"2021-08-02T13:57:45.976374Z","shell.execute_reply":"2021-08-02T13:57:45.981182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.model_avg_weights = [i[1]['weight'] for i in models_map.items()]\ncfg.model_avg_weights","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:46.973695Z","iopub.execute_input":"2021-08-02T13:57:46.974042Z","iopub.status.idle":"2021-08-02T13:57:46.982031Z","shell.execute_reply.started":"2021-08-02T13:57:46.97401Z","shell.execute_reply":"2021-08-02T13:57:46.981053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.abs(np.array(cfg.model_avg_weights).sum() - 1.0)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:49.169105Z","iopub.execute_input":"2021-08-02T13:57:49.169514Z","iopub.status.idle":"2021-08-02T13:57:49.179239Z","shell.execute_reply.started":"2021-08-02T13:57:49.16948Z","shell.execute_reply":"2021-08-02T13:57:49.178296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_items = models_map.items()\n\nif not (np.abs(np.array(cfg.model_avg_weights).sum() - 1.0) < 1e-8):\n    default_weight = 1 / len(model_items)\n    for k, v in model_items:\n        v['weight'] = default_weight\n        \ncfg.model_avg_weights = [i[1]['weight'] for i in models_map.items()]\ncfg.model_avg_weights","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:52.167918Z","iopub.execute_input":"2021-08-02T13:57:52.168275Z","iopub.status.idle":"2021-08-02T13:57:52.175622Z","shell.execute_reply.started":"2021-08-02T13:57:52.168241Z","shell.execute_reply":"2021-08-02T13:57:52.174701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(models_map) == len(cfg.model_avg_weights), f\"Length of models map ({len(models_map)}) is not the same as averaging weights ({len(cfg.model_avg_weights)})\"\nassert np.abs(np.array(cfg.model_avg_weights).sum() - 1.0) < 1e-8, f'{np.sum(cfg.model_avg_weights)}'","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:52.741779Z","iopub.execute_input":"2021-08-02T13:57:52.742141Z","iopub.status.idle":"2021-08-02T13:57:52.746825Z","shell.execute_reply.started":"2021-08-02T13:57:52.742087Z","shell.execute_reply":"2021-08-02T13:57:52.745777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH_BART}","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:53.213775Z","iopub.execute_input":"2021-08-02T13:57:53.214113Z","iopub.status.idle":"2021-08-02T13:57:53.920377Z","shell.execute_reply.started":"2021-08-02T13:57:53.214065Z","shell.execute_reply":"2021-08-02T13:57:53.919258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH_ROBERTA_LARGE}","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:53.924074Z","iopub.execute_input":"2021-08-02T13:57:53.926885Z","iopub.status.idle":"2021-08-02T13:57:54.636916Z","shell.execute_reply.started":"2021-08-02T13:57:53.926837Z","shell.execute_reply":"2021-08-02T13:57:54.635843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH_ROBERTA_BALANCED}","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:57:54.640625Z","iopub.execute_input":"2021-08-02T13:57:54.64089Z","iopub.status.idle":"2021-08-02T13:57:55.286511Z","shell.execute_reply.started":"2021-08-02T13:57:54.64086Z","shell.execute_reply":"2021-08-02T13:57:55.285439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH_DEBERTA_LARGE}","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:46.301084Z","iopub.execute_input":"2021-07-27T14:35:46.301464Z","iopub.status.idle":"2021-07-27T14:35:46.940449Z","shell.execute_reply.started":"2021-07-27T14:35:46.301423Z","shell.execute_reply":"2021-07-27T14:35:46.939438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH_ROBERTA_LIGHT_2}","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:46.943555Z","iopub.execute_input":"2021-07-27T14:35:46.943845Z","iopub.status.idle":"2021-07-27T14:35:47.579318Z","shell.execute_reply.started":"2021-07-27T14:35:46.943813Z","shell.execute_reply":"2021-07-27T14:35:47.578481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH_T5_LARGE}","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:47.580888Z","iopub.execute_input":"2021-07-27T14:35:47.58137Z","iopub.status.idle":"2021-07-27T14:35:48.219484Z","shell.execute_reply.started":"2021-07-27T14:35:47.581328Z","shell.execute_reply":"2021-07-27T14:35:48.218583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cls_light_embeddings(dl, transformer_model):\n    cls_embeddings = []\n    with torch.no_grad():\n        for input_features in tqdm(dl, total=len(dl)):\n            _, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n            embedding_out = context_vector.detach().cpu().numpy()\n            cls_embeddings.extend(embedding_out)\n    return np.array(cls_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.222197Z","iopub.execute_input":"2021-07-27T14:35:48.222578Z","iopub.status.idle":"2021-07-27T14:35:48.228854Z","shell.execute_reply.started":"2021-07-27T14:35:48.222532Z","shell.execute_reply":"2021-07-27T14:35:48.227765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    \n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        \n        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n        self.final_layer = nn.Linear(hidden_dim, num_targets)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.hidden_layer(features))\n        score = self.final_layer(att)\n        attention_weights = torch.softmax(score, dim=1)\n        return attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.23073Z","iopub.execute_input":"2021-07-27T14:35:48.231081Z","iopub.status.idle":"2021-07-27T14:35:48.239772Z","shell.execute_reply.started":"2021-07-27T14:35:48.231046Z","shell.execute_reply":"2021-07-27T14:35:48.238925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionLargeHead(nn.Module):\n    \n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        \n        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n        self.final_layer = nn.Linear(hidden_dim, num_targets)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.hidden_layer(features))\n        score = self.final_layer(att)\n        attention_weights = torch.softmax(score, dim=1)\n        return attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.241262Z","iopub.execute_input":"2021-07-27T14:35:48.24166Z","iopub.status.idle":"2021-07-27T14:35:48.249651Z","shell.execute_reply.started":"2021-07-27T14:35:48.241623Z","shell.execute_reply":"2021-07-27T14:35:48.248867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nclass CommonLitElectraModel(nn.Module):\n    def __init__(self, config_path=cfg.pretrained_electra_large_lm_model):\n        super(CommonLitElectraModel, self).__init__()\n        config = AutoConfig.from_pretrained(config_path)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(config_path, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n        last_layer_hidden_states = hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_cls_light_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.251288Z","iopub.execute_input":"2021-07-27T14:35:48.251804Z","iopub.status.idle":"2021-07-27T14:35:48.262937Z","shell.execute_reply.started":"2021-07-27T14:35:48.251767Z","shell.execute_reply":"2021-07-27T14:35:48.262198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5EncoderModel\n\nclass CommonLitT5Model(nn.Module):\n    def __init__(self):\n        super(CommonLitT5Model, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.pretrained_t5_large_lm_model)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = T5EncoderModel.from_pretrained(cfg.pretrained_t5_large_lm_model, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_cls_light_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.265849Z","iopub.execute_input":"2021-07-27T14:35:48.2661Z","iopub.status.idle":"2021-07-27T14:35:48.276577Z","shell.execute_reply.started":"2021-07-27T14:35:48.266069Z","shell.execute_reply":"2021-07-27T14:35:48.275711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import PreTrainedModel, AutoConfig\n\nclass CommonLitBartModel(nn.Module):\n    def __init__(self):\n        super(CommonLitBartModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.pretrained_bart_lm_model)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_bart_lm_model, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_cls_light_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.277902Z","iopub.execute_input":"2021-07-27T14:35:48.278349Z","iopub.status.idle":"2021-07-27T14:35:48.289111Z","shell.execute_reply.started":"2021-07-27T14:35:48.27831Z","shell.execute_reply":"2021-07-27T14:35:48.288295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionSimpleHead(nn.Module):\n    \n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        \n        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n        self.final_layer = nn.Linear(hidden_dim, num_targets)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.hidden_layer(features))\n        score = self.final_layer(att)\n        \n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.290263Z","iopub.execute_input":"2021-07-27T14:35:48.2908Z","iopub.status.idle":"2021-07-27T14:35:48.302282Z","shell.execute_reply.started":"2021-07-27T14:35:48.29076Z","shell.execute_reply":"2021-07-27T14:35:48.301406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_embeddings(dl, transformer_model):\n    cls_embeddings = []\n    hidden_size = 1024 if str(type(inference_model)).find('Bart') > -1 else 768\n    attention_head = AttentionSimpleHead(hidden_size, hidden_size, 1).cuda()\n    with torch.no_grad():\n        for input_features in tqdm(dl, total=len(dl)):\n            output = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda(), output_hidden_states=True)\n            last_hidden_state = output['last_hidden_state']\n            hidden_state_name = 'decoder_hidden_states' if 'decoder_hidden_states' in output else 'hidden_states'\n            previous_hidden_state = output[hidden_state_name][-2]\n            cls_state = last_hidden_state[:,0,:].detach().cpu().numpy()\n            cls_state_2 = attention_head(last_hidden_state).detach().cpu().numpy()\n            cls_state_3 = previous_hidden_state[:,0,:].detach().cpu().numpy()\n            concatenated = np.average([cls_state, cls_state_2, cls_state_3], axis=0)\n            cls_embeddings.extend(concatenated)\n    return np.array(cls_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.304361Z","iopub.execute_input":"2021-07-27T14:35:48.304839Z","iopub.status.idle":"2021-07-27T14:35:48.313864Z","shell.execute_reply.started":"2021-07-27T14:35:48.304799Z","shell.execute_reply":"2021-07-27T14:35:48.312969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitRobertaLargeModel(nn.Module):\n    def __init__(self):\n        super(CommonLitRobertaLargeModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.pretrained_roberta_large_lm_model)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_roberta_large_lm_model, config=config)\n        self.attention = AttentionLargeHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_cls_light_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.529789Z","iopub.execute_input":"2021-07-27T14:35:48.530062Z","iopub.status.idle":"2021-07-27T14:35:48.538268Z","shell.execute_reply.started":"2021-07-27T14:35:48.530035Z","shell.execute_reply":"2021-07-27T14:35:48.537388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitRobertaModel(PreTrainedModel):\n    def __init__(self):\n        super(PreTrainedModel, self).__init__()\n        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_roberta_lm_model)\n        self.drop = nn.Dropout(0.5)\n        self.config = AutoConfig.from_pretrained(cfg.pretrained_roberta_lm_model)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.out = nn.Linear(self.config.hidden_size, 1)\n        self.transformer_model.get_cls_embeddings = self.get_cls_embeddings\n#         self._init_weights(self.layer_norm)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        if isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer_model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), output_hidden_states=False)\n        x = transformer_out.pooler_output\n#         x = transformer_out.last_hidden_state[:, 0, :] # N, C, X\n        x = self.layer_norm(x)\n        x = self.drop(x)\n        x = self.out(x)\n        return x\n    \n    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n        \"\"\"\n        For models that inherit from :class:`~transformers.PreTrainedModel`, uses that method to compute the number of\n        floating point operations for every backward + forward pass. If using another model, either implement such a\n        method in the model or subclass and override this method.\n        Args:\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n        Returns:\n            :obj:`int`: The number of floating-point operations.\n        \"\"\"\n        return 0\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:48.810928Z","iopub.execute_input":"2021-07-27T14:35:48.811192Z","iopub.status.idle":"2021-07-27T14:35:48.822512Z","shell.execute_reply.started":"2021-07-27T14:35:48.811165Z","shell.execute_reply":"2021-07-27T14:35:48.821562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitRobertaStandardizedModel(PreTrainedModel):\n    def __init__(self):\n        super(PreTrainedModel, self).__init__()\n        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_roberta_standardized_lm_model)\n        self.drop = nn.Dropout(0.5)\n        self.config = AutoConfig.from_pretrained(cfg.pretrained_roberta_standardized_lm_model)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.out = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self.transformer_model.get_cls_embeddings = self.get_cls_embeddings\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        if isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer_model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), \n                                                 output_hidden_states=True)\n#         x = transformer_out.pooler_output\n#         x = transformer_out.last_hidden_state[:, 0, :] # N, C, X\n        x = torch.mean(transformer_out.hidden_states[-2], axis=1)\n#         x2 = torch.mean(transformer_out.encoder_hidden_states[-2], axis=1)\n#         x = torch.cat([x, x2], axis=1)\n        x = self.layer_norm(x)\n        x = self.drop(x)\n        x = self.out(x)\n        return x\n    \n    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n        \"\"\"\n        For models that inherit from :class:`~transformers.PreTrainedModel`, uses that method to compute the number of\n        floating point operations for every backward + forward pass. If using another model, either implement such a\n        method in the model or subclass and override this method.\n        Args:\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n        Returns:\n            :obj:`int`: The number of floating-point operations.\n        \"\"\"\n        return 0\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:49.081516Z","iopub.execute_input":"2021-07-27T14:35:49.082199Z","iopub.status.idle":"2021-07-27T14:35:49.104339Z","shell.execute_reply.started":"2021-07-27T14:35:49.082152Z","shell.execute_reply":"2021-07-27T14:35:49.10341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitLightModel(nn.Module):\n    def __init__(self, lm_model=cfg.pretrained_roberta_large_lm_model):\n        super(CommonLitLightModel, self).__init__()\n        config = AutoConfig.from_pretrained(lm_model)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = AutoModel.from_pretrained(lm_model, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_cls_light_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:49.474363Z","iopub.execute_input":"2021-07-27T14:35:49.474674Z","iopub.status.idle":"2021-07-27T14:35:49.482727Z","shell.execute_reply.started":"2021-07-27T14:35:49.474643Z","shell.execute_reply":"2021-07-27T14:35:49.48189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nclass CommonLitDebertaXLargeModel(nn.Module):\n    def __init__(self):\n        super(CommonLitDebertaXLargeModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.pretrained_deberta_xlarge_model)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = AutoModelForSequenceClassification.from_pretrained(cfg.pretrained_deberta_xlarge_model, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['hidden_states']\n        last_layer_hidden_states = hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector\n    \n    def get_cls_embeddings(self, dl, transformer_model):\n        return get_cls_light_embeddings(dl, transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:49.838825Z","iopub.execute_input":"2021-07-27T14:35:49.839121Z","iopub.status.idle":"2021-07-27T14:35:49.847655Z","shell.execute_reply.started":"2021-07-27T14:35:49.83909Z","shell.execute_reply":"2021-07-27T14:35:49.846773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model_name='CommonLitBartModel', model_path=MODELS_PATH_BART, i=0):\n    if model_path in [MODELS_PATH_ROBERTA_LARGE, MODELS_PATH_ROBERTA_LIGHT_2, MODELS_PATH_BART, \n                      MODELS_PATH_DEBERTA_LARGE, MODELS_PATH_T5_LARGE, MODELS_PATH_ELECTRA_LARGE,\n                     MODELS_PATH_DEBERTA_UNIFORM, MODELS_PATH_DEBERTA_XLARGE]:\n        i = i + 1\n    print(f'Model {model_name} {i}')\n    if model_path == MODELS_PATH_ROBERTA_LIGHT_2:\n        inference_model = CommonLitLightModel(cfg.pretrained_roberta_light_2_lm_model)\n    elif model_path == MODELS_PATH_ROBERTA_BALANCED:\n        inference_model = CommonLitLightModel(cfg.pretrained_roberta_light_balanced_lm_model)\n    elif model_path == MODELS_PATH_ROBERTA_BASE_LIGHT:\n        inference_model = CommonLitLightModel(cfg.pretrained_roberta_base_light_lm_model)\n    elif model_path == MODELS_PATH_DEBERTA_LARGE:\n        inference_model = CommonLitLightModel(cfg.pretrained_deberta_large_lm_model)\n    elif model_path == MODELS_PATH_T5_LARGE:\n        inference_model = CommonLitT5Model()\n    elif model_path == MODELS_PATH_ELECTRA_LARGE:\n        inference_model = CommonLitElectraModel(config_path=cfg.pretrained_electra_large_lm_model)\n    elif model_path == MODELS_PATH_DEBERTA_UNIFORM:\n        inference_model = CommonLitLightModel(cfg.pretrained_deberta_uniform_model)\n    elif model_path == MODELS_PATH_DEBERTA_XLARGE:\n        inference_model = CommonLitDebertaXLargeModel()\n    else:\n        inference_model = eval(f'{model_name}()')\n    inference_model = inference_model.cuda()\n    bin_file = f'{i}_pytorch_model.bin'\n    print(f'Loading model from {model_path/bin_file}')\n    inference_model.load_state_dict(torch.load(str(model_path/bin_file)))\n    inference_model.eval();\n    return inference_model","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:50.1113Z","iopub.execute_input":"2021-07-27T14:35:50.111591Z","iopub.status.idle":"2021-07-27T14:35:50.120671Z","shell.execute_reply.started":"2021-07-27T14:35:50.111562Z","shell.execute_reply":"2021-07-27T14:35:50.119733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataSet and Tokenizers","metadata":{}},{"cell_type":"code","source":"def convert_to_list(t):\n    return t.flatten().long()\n\nclass CommonLitDataset(nn.Module):\n    def __init__(self, text, test_id, tokenizer, max_len=128):\n        self.excerpt = text\n        self.test_id = test_id\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return {'input_ids': convert_to_list(encode['input_ids']),\n                'attention_mask': convert_to_list(encode['attention_mask']),\n                'id': self.test_id[idx]}\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:50.975321Z","iopub.execute_input":"2021-07-27T14:35:50.975638Z","iopub.status.idle":"2021-07-27T14:35:50.982505Z","shell.execute_reply.started":"2021-07-27T14:35:50.975607Z","shell.execute_reply":"2021-07-27T14:35:50.981562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, BartTokenizer, DebertaTokenizer, T5Tokenizer, ElectraTokenizer\n\ndef load_tokenizer(model_path=MODELS_PATH_BART, i=0):\n    if model_path in [MODELS_PATH_ROBERTA_LARGE, MODELS_PATH_ROBERTA_LIGHT_2, MODELS_PATH_DEBERTA_LARGE, \n                      MODELS_PATH_BART, MODELS_PATH_T5_LARGE, MODELS_PATH_ELECTRA_LARGE, \n                      MODELS_PATH_DEBERTA_UNIFORM, MODELS_PATH_DEBERTA_XLARGE]:\n        i = i + 1\n    tokenizer_path =  model_path/f'tokenizer-{i}'\n    if model_path in [MODELS_PATH_ROBERTA_LARGE, MODELS_PATH_ROBERTA_LIGHT_2]:\n        print('Tokenizer', tokenizer_path)\n        tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n    elif model_path == MODELS_PATH_BART:\n        tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n    elif model_path == MODELS_PATH_DEBERTA_LARGE:\n        tokenizer = DebertaTokenizer.from_pretrained(tokenizer_path)\n    elif model_path == MODELS_PATH_T5_LARGE:\n        tokenizer = T5Tokenizer.from_pretrained(str(tokenizer_path))\n    elif model_path == MODELS_PATH_ELECTRA_LARGE:\n        tokenizer = ElectraTokenizer.from_pretrained(str(tokenizer_path))\n    elif model_path == MODELS_PATH_DEBERTA_UNIFORM:\n        tokenizer = DebertaTokenizer.from_pretrained(str(tokenizer_path))\n    elif model_path == MODELS_PATH_DEBERTA_XLARGE:\n        tokenizer = DebertaTokenizer.from_pretrained(str(tokenizer_path))\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:54.610535Z","iopub.execute_input":"2021-07-27T14:35:54.61191Z","iopub.status.idle":"2021-07-27T14:35:54.62628Z","shell.execute_reply.started":"2021-07-27T14:35:54.611864Z","shell.execute_reply":"2021-07-27T14:35:54.625245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dl(df, tokenizer):\n    text = df['excerpt'].values\n    ids = df['id'].values\n    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.max_len)\n    return DataLoader(ds, \n                      batch_size = cfg.batch_size,\n                      shuffle=False,\n                      num_workers = 1,\n                      pin_memory=True,\n                      drop_last=False\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:55.562396Z","iopub.execute_input":"2021-07-27T14:35:55.562797Z","iopub.status.idle":"2021-07-27T14:35:55.567567Z","shell.execute_reply.started":"2021-07-27T14:35:55.562763Z","shell.execute_reply":"2021-07-27T14:35:55.566774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extract Number of Bins","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:01:49.420156Z","iopub.execute_input":"2021-05-27T10:01:49.420515Z","iopub.status.idle":"2021-05-27T10:01:49.424629Z","shell.execute_reply.started":"2021-05-27T10:01:49.420481Z","shell.execute_reply":"2021-05-27T10:01:49.423522Z"}}},{"cell_type":"code","source":"num_bins = int(np.ceil(np.log2(len(train_df))))\ntrain_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\nbins = train_df['bins'].values","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:56.490594Z","iopub.execute_input":"2021-07-27T14:35:56.49095Z","iopub.status.idle":"2021-07-27T14:35:56.502641Z","shell.execute_reply.started":"2021-07-27T14:35:56.490917Z","shell.execute_reply":"2021-07-27T14:35:56.501785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"def rmse_score(X, y):\n    return np.sqrt(mean_squared_error(X, y))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:57.521866Z","iopub.execute_input":"2021-07-27T14:35:57.522182Z","iopub.status.idle":"2021-07-27T14:35:57.526702Z","shell.execute_reply.started":"2021-07-27T14:35:57.522152Z","shell.execute_reply":"2021-07-27T14:35:57.525492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)\n\ndef calc_avg(scores, weights):\n    return np.average(np.array(scores), weights=weights, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:57.8198Z","iopub.execute_input":"2021-07-27T14:35:57.820097Z","iopub.status.idle":"2021-07-27T14:35:57.825041Z","shell.execute_reply.started":"2021-07-27T14:35:57.820068Z","shell.execute_reply":"2021-07-27T14:35:57.823964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_infer(inference_model, tokenizer):\n    test_dl = create_dl(test_df, tokenizer)\n    train_dl = create_dl(train_df, tokenizer)\n    if isinstance(inference_model, (CommonLitLightModel, CommonLitRobertaLargeModel, \n                                    CommonLitBartModel, CommonLitT5Model, CommonLitElectraModel, CommonLitDebertaXLargeModel)):\n        transformer_model = inference_model\n    elif hasattr(inference_model, 'transformer_model'):\n        transformer_model = inference_model.transformer_model\n    else:\n        transformer_model = inference_model\n    transformer_model.cuda()\n    X = transformer_model.get_cls_embeddings(train_dl, transformer_model)\n    y = train_df['target'].values\n    X_test = transformer_model.get_cls_embeddings(test_dl, transformer_model)\n    kfold = StratifiedKFold(n_splits=cfg.n_folds)\n    scores = []\n    rmse_scores = []\n    for kernel in cfg.svm_kernels:\n        print('* Kernel', kernel, ' *')\n        kernel_scores = []\n        kernel_rmse_scores = []\n        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n            print('Fold', k, train_idx.shape, valid_idx.shape)\n            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_valid, y_valid = X[valid_idx], y[valid_idx]\n            model.fit(X_train, y_train)\n            prediction = model.predict(X_valid)\n            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n            print('rmse_score', kernel_rmse_scores[k])\n            kernel_scores.append(model.predict(X_test))\n        scores.append(calc_mean(kernel_scores))\n        rmse_scores.append(calc_mean(kernel_rmse_scores))\n    \n    del X_test\n    del X\n    del y\n    del kfold\n    del test_dl\n    del train_dl\n    \n    return scores, rmse_scores","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:35:58.246076Z","iopub.execute_input":"2021-07-27T14:35:58.246389Z","iopub.status.idle":"2021-07-27T14:35:58.258206Z","shell.execute_reply.started":"2021-07-27T14:35:58.246357Z","shell.execute_reply":"2021-07-27T14:35:58.25716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfinal_scores = []\nfinal_rmse_scores = []\nall_preds = []\nfor i in range(cfg.model_offset, cfg.model_limit):\n    print(f'\\n#### Round {i} ####\\n')\n    model_scores = []\n    model_rmse_scores = []\n    for (_, model_data) in models_map.items():\n        print(f\"#### {i} - {model_data['model']} ####\")\n        path = model_data['tokenizer_path']\n        inference_model = load_model(model_name=model_data['model'], model_path=path, i=i)\n        tokenizer = load_tokenizer(model_path=path, i=i)\n        scores, rmse_scores = train_infer(inference_model, tokenizer)\n        scores = calc_mean(scores)\n        all_preds.append(scores)\n        rmse_scores = calc_mean(rmse_scores)\n        model_scores.append(scores)\n        model_rmse_scores.append(rmse_scores)\n        print('scores shape', np.array(scores).shape)\n        print('rmse_scores shape', np.array(rmse_scores).shape)\n        del inference_model\n        torch.cuda.empty_cache()\n        del tokenizer\n        gc.collect()\n    final_scores.append(np.average(model_scores, weights=cfg.model_avg_weights, axis=0))\n    final_rmse_scores.append(calc_mean(model_rmse_scores))\nprint('FINAL RMSE score', calc_mean(final_rmse_scores))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T14:36:09.721399Z","iopub.execute_input":"2021-07-27T14:36:09.721735Z","iopub.status.idle":"2021-07-27T15:40:28.533537Z","shell.execute_reply.started":"2021-07-27T14:36:09.7217Z","shell.execute_reply":"2021-07-27T15:40:28.532428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ensure the mean of the prediction equals the mean of the training data","metadata":{}},{"cell_type":"markdown","source":"Calculate the weights based on the rmse score","metadata":{}},{"cell_type":"code","source":"final_rmse_scores, final_scores","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.535272Z","iopub.execute_input":"2021-07-27T15:40:28.535636Z","iopub.status.idle":"2021-07-27T15:40:28.545615Z","shell.execute_reply.started":"2021-07-27T15:40:28.535596Z","shell.execute_reply":"2021-07-27T15:40:28.544653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_rmse_score_mean_array = np.array(final_rmse_scores)\nkernel_rmse_score_mean_sum = np.sum(kernel_rmse_score_mean_array)\nprop_losses = kernel_rmse_score_mean_array / kernel_rmse_score_mean_sum\nprop_losses_sum = (1 - prop_losses).sum()\nweights = (1 - prop_losses) / prop_losses_sum\nweights","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.547684Z","iopub.execute_input":"2021-07-27T15:40:28.548096Z","iopub.status.idle":"2021-07-27T15:40:28.557446Z","shell.execute_reply.started":"2021-07-27T15:40:28.548059Z","shell.execute_reply":"2021-07-27T15:40:28.556302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights.sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.559853Z","iopub.execute_input":"2021-07-27T15:40:28.560277Z","iopub.status.idle":"2021-07-27T15:40:28.566538Z","shell.execute_reply.started":"2021-07-27T15:40:28.560239Z","shell.execute_reply":"2021-07-27T15:40:28.565444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_mean = train_df['target'].mean()\nfinal_scores_flat = calc_avg(final_scores, weights).flatten()\nfinal_scores_mean = final_scores_flat.mean()\ntarget_mean, np.array(all_preds).flatten().mean(), np.array(final_scores).mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.568268Z","iopub.execute_input":"2021-07-27T15:40:28.568995Z","iopub.status.idle":"2021-07-27T15:40:28.579173Z","shell.execute_reply.started":"2021-07-27T15:40:28.568953Z","shell.execute_reply":"2021-07-27T15:40:28.577942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_diff = target_mean - final_scores_mean\nmean_diff, mean_diff / len(final_scores)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.580558Z","iopub.execute_input":"2021-07-27T15:40:28.581118Z","iopub.status.idle":"2021-07-27T15:40:28.587863Z","shell.execute_reply.started":"2021-07-27T15:40:28.58108Z","shell.execute_reply":"2021-07-27T15:40:28.586866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df['target'] = final_scores_flat + mean_diff / len(final_scores)\n# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\nsample_df","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.589386Z","iopub.execute_input":"2021-07-27T15:40:28.590031Z","iopub.status.idle":"2021-07-27T15:40:28.605847Z","shell.execute_reply.started":"2021-07-27T15:40:28.589994Z","shell.execute_reply":"2021-07-27T15:40:28.604897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df['target'].mean() # Should be very close to the train mean","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.608259Z","iopub.execute_input":"2021-07-27T15:40:28.608631Z","iopub.status.idle":"2021-07-27T15:40:28.614614Z","shell.execute_reply.started":"2021-07-27T15:40:28.608592Z","shell.execute_reply":"2021-07-27T15:40:28.613437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(sample_df).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.616489Z","iopub.execute_input":"2021-07-27T15:40:28.617046Z","iopub.status.idle":"2021-07-27T15:40:28.8913Z","shell.execute_reply.started":"2021-07-27T15:40:28.617004Z","shell.execute_reply":"2021-07-27T15:40:28.890434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-07-27T15:40:28.89254Z","iopub.execute_input":"2021-07-27T15:40:28.892914Z","iopub.status.idle":"2021-07-27T15:40:29.697387Z","shell.execute_reply.started":"2021-07-27T15:40:28.892875Z","shell.execute_reply":"2021-07-27T15:40:29.696367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
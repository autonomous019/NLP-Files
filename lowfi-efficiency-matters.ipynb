{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nimport os\nfrom types import SimpleNamespace\nimport yaml\nimport multiprocessing as mp\nfrom glob import glob\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport collections\nimport lightgbm\n     \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n#this work is based on https://www.kaggle.com/code/philippsinger/team-hydrogen-efficiency-prize-1st-place\n#data for original version https://www.kaggle.com/code/ryanholbrook/feedback-prize-efficiency-leaderboard/data\n\n\n#built for \"baby, ain't got no GPU, damn!\"\n\n\n# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-15T21:44:29.948758Z","iopub.execute_input":"2022-09-15T21:44:29.949278Z","iopub.status.idle":"2022-09-15T21:44:32.861571Z","shell.execute_reply.started":"2022-09-15T21:44:29.949166Z","shell.execute_reply":"2022-09-15T21:44:32.860008Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\n\n# ====================================================\n# CFG\n# ====================================================\nEXP_NAME = \"feedback-prize-english-language-learning\"\n\nclass CFG:\n    my_device=device\n    competition='FB3'\n    debug=False\n    num_workers=4\n    model=\"microsoft/deberta-v3-base\"\n    regression_lr=1e-6\n    max_len=512 #originally 512\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=1916\n    #cache_dir = f\"../input/{EXP_NAME}/deberta-v3-base/\"\n    cache_dir = f\"../input/debertav3base-models/deberta-v3-base/\"  \n    _tokenizer_sep_token = \"\"\n    _tokenizer_start_token_id = []\n    _tokenizer_end_token_id = []\n    _tokenizer_size = \"\"\n\n    \n#if CFG.debug:\n    #some debug settings here","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:44:38.190975Z","iopub.execute_input":"2022-09-15T21:44:38.191400Z","iopub.status.idle":"2022-09-15T21:44:38.200746Z","shell.execute_reply.started":"2022-09-15T21:44:38.191366Z","shell.execute_reply":"2022-09-15T21:44:38.199012Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"N_CORES = mp.cpu_count()\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    data_folder = \"test\"\n    df = pd.read_csv(\"../input/feedback-prize-english-language-learning/test.csv\")\n    CALC_SCORE = False\n    print(\"testing\")\n    print(df.head())\nelse:\n    data_folder = \"train\"\n    df = pd.read_csv(\"../input/feedback-prize-english-language-learning/train.csv\")\n    ids = df.text_id.unique()\n    np.random.seed(CFG.seed)\n    val_ids = np.random.choice(ids, size=3000, replace=False)\n    df = df[df.text_id.isin(val_ids)]\n    df = df.reset_index(drop=True)\n    CALC_SCORE = True\n    print(\"training\")\n    print(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:44:43.557031Z","iopub.execute_input":"2022-09-15T21:44:43.557457Z","iopub.status.idle":"2022-09-15T21:44:43.789429Z","shell.execute_reply.started":"2022-09-15T21:44:43.557422Z","shell.execute_reply":"2022-09-15T21:44:43.787897Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"training\n        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n4  004AC288D833  Dear Principal,\\r\\n\\r\\nOur school should have ...       3.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  \n0     3.5         3.0          3.0      4.0          3.0  \n1     2.5         3.0          2.0      2.0          2.5  \n2     3.5         3.0          3.0      3.0          2.5  \n3     3.0         3.0          3.0      2.5          2.5  \n4     4.0         4.0          3.5      3.5          4.0  \n","output_type":"stream"}]},{"cell_type":"code","source":"\nfull_texts = df.loc[:,[\"text_id\",\"full_text\"]]\nprint(full_texts)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:44:48.815499Z","iopub.execute_input":"2022-09-15T21:44:48.815922Z","iopub.status.idle":"2022-09-15T21:44:48.827038Z","shell.execute_reply.started":"2022-09-15T21:44:48.815889Z","shell.execute_reply":"2022-09-15T21:44:48.826050Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"           text_id                                          full_text\n0     0016926B079C  I think that students would benefit from learn...\n1     0022683E9EA5  When a problem is a change you have to let it ...\n2     00299B378633  Dear, Principal\\n\\nIf u change the school poli...\n3     0049B1DF5CCC  Small act of kindness can impact in other peop...\n4     004AC288D833  Dear Principal,\\r\\n\\r\\nOur school should have ...\n...            ...                                                ...\n2995  FFAEAF8D0C90  Soccer, all people like to play soccer, and ot...\n2996  FFCDB2524616  I agree with Ralph Waldo Emerson's \"\\n\\nTo be ...\n2997  FFD29828A873  I believe using cellphones in class for educat...\n2998  FFE16D704B16  Many people disagree with Albert Schweitzer's ...\n2999  FFED00D6E0BD  Do you think that failure is the main thing fo...\n\n[3000 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"#tokenizer = AutoTokenizer.from_pretrained(CFG.model)\ntokenizer = AutoTokenizer.from_pretrained(CFG.cache_dir)\n\nCFG._tokenizer_sep_token = tokenizer.sep_token\n\nCFG._tokenizer_start_token_id = []\nCFG._tokenizer_end_token_id = []\n\nd_types = sorted(df.syntax.unique()) #was discourse types\n\nfor t in d_types:\n    tokenizer.add_tokens([f\"[START_{t}]\"], special_tokens=True)\n    CFG._tokenizer_start_token_id.append(tokenizer.encode(f\"[START_{t}]\")[1])\n    \nfor t in d_types:\n    tokenizer.add_tokens([f\"[END_{t}]\"], special_tokens=True)\n    CFG._tokenizer_end_token_id.append(tokenizer.encode(f\"[END_{t}]\")[1])\n\ntokenizer.add_tokens([f\"\\n\"], special_tokens=True)\nCFG._tokenizer_size = len(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:44:53.259525Z","iopub.execute_input":"2022-09-15T21:44:53.260423Z","iopub.status.idle":"2022-09-15T21:44:54.822115Z","shell.execute_reply.started":"2022-09-15T21:44:53.260378Z","shell.execute_reply":"2022-09-15T21:44:54.820765Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"grps = df.groupby(\"text_id\", sort=False)\ngrp_texts = []\n\nfor grp in grps.groups:\n    g = grps.get_group(grp)\n    t = g.full_text.values[0]\n\n    end = 0\n    for j in range(len(g)):\n        d = g.full_text.values[j]\n        start = t[end:].find(d.strip())\n        start = start + end\n\n        end = start + len(d.strip())\n        t = (\n            t[:start]\n            + f\" [START_{g.full_text.values[j]}]  \"\n            + t[start:end]\n            + f\" [END_{g.full_text.values[j]}] \"\n            + t[end:]\n        )\n\n    t = \" \".join(g.full_text.values) + f\" {CFG._tokenizer_sep_token} \" + t\n    grp_texts.append(t)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:44:59.657587Z","iopub.execute_input":"2022-09-15T21:44:59.658560Z","iopub.status.idle":"2022-09-15T21:45:00.591418Z","shell.execute_reply.started":"2022-09-15T21:44:59.658510Z","shell.execute_reply":"2022-09-15T21:45:00.590037Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def encode(text):\n    sample = dict()\n    encodings = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n        #max_length=cfg.tokenizer.max_length,\n        max_length=CFG.max_len,\n    )\n    sample[\"input_ids\"] = encodings[\"input_ids\"][0]\n    sample[\"attention_mask\"] = encodings[\"attention_mask\"][0]\n    return sample","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:45:05.309881Z","iopub.execute_input":"2022-09-15T21:45:05.310338Z","iopub.status.idle":"2022-09-15T21:45:05.317585Z","shell.execute_reply.started":"2022-09-15T21:45:05.310304Z","shell.execute_reply":"2022-09-15T21:45:05.316126Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"pool_obj = mp.Pool(N_CORES)\ngrp_texts = pool_obj.map(encode, grp_texts)\n\nlens = [torch.sum(x[\"attention_mask\"]).item() for x in grp_texts]\n\nlens_map = df[[\"text_id\"]].drop_duplicates()\nlens_map[\"count\"] = lens\nlens_map[\"orig_text_order\"] = range(len(lens_map))\n\ndf[\"orig_order\"] = range(len(df))\ndf = df.merge(lens_map)\ndf = df.sort_values([\"count\", \"text_id\", \"orig_order\"], ascending=True).reset_index(drop=True)\ngrp_texts = [grp_texts[i] for i in df[\"orig_text_order\"].unique()]","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:45:10.114925Z","iopub.execute_input":"2022-09-15T21:45:10.115380Z","iopub.status.idle":"2022-09-15T21:45:21.898124Z","shell.execute_reply.started":"2022-09-15T21:45:10.115341Z","shell.execute_reply":"2022-09-15T21:45:21.896284Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset(Dataset):\n    def __init__(self, grp_texts):\n        self.grp_texts = grp_texts\n\n    def __len__(self):\n        return len(self.grp_texts)\n\n    def batch_to_device(batch, device):\n        if isinstance(batch, torch.Tensor):\n            return batch.to(device)\n        elif isinstance(batch, collections.abc.Mapping):\n            return {\n                key: FeedbackDataset.batch_to_device(value, device)\n                for key, value in batch.items()\n            }\n\n    def __getitem__(self, idx):\n        sample = self.grp_texts[idx]\n        if idx == 0:\n            print(sample)\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:45:29.612633Z","iopub.execute_input":"2022-09-15T21:45:29.613143Z","iopub.status.idle":"2022-09-15T21:45:29.623024Z","shell.execute_reply.started":"2022-09-15T21:45:29.613090Z","shell.execute_reply":"2022-09-15T21:45:29.621959Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class NLPAllclsTokenPooling(nn.Module):\n    def __init__(self, dim):\n        super(NLPAllclsTokenPooling, self).__init__()\n\n        self.dim = dim\n        self.feat_mult = 3\n\n    def forward(self, x, attention_mask, input_ids, cfg):\n        ret = []\n        for j in range(x.shape[0]):\n            idx0 = torch.where(\n                (input_ids[j] >= min(CFG._tokenizer_start_token_id))\n                & (input_ids[j] <= max(CFG._tokenizer_start_token_id))\n            )[0]\n            idx1 = torch.where(\n                (input_ids[j] >= min(CFG._tokenizer_end_token_id))\n                & (input_ids[j] <= max(CFG._tokenizer_end_token_id))\n            )[0]\n\n            xx = []\n            for jj in range(len(idx0)):\n                xx0 = x[j, idx0[jj]]\n                xx1 = x[j, idx1[jj]]\n                xx2 = x[j, idx0[jj] + 1 : idx1[jj]].mean(dim=0)\n                xxx = torch.cat([xx0, xx1, xx2]).unsqueeze(0)\n                xx.append(xxx)\n            xx = torch.cat(xx)\n            ret.append(xx)\n\n        return ret\n\n\nclass FeedbackModel(nn.Module):\n    def __init__(self, cfg):\n        super(FeedbackModel, self).__init__()\n\n        self.cfg = cfg\n        self.n_classes = 3\n        #config = AutoConfig.from_pretrained(CFG.model)\n        config = AutoConfig.from_pretrained(CFG.cache_dir)\n        self.backbone = AutoModel.from_config(config)\n        self.backbone.pooler = None\n        self.backbone.resize_token_embeddings(CFG._tokenizer_size)\n\n        self.pooling = NLPAllclsTokenPooling(\n            dim=1\n        )  # init pooling and pool over token dimension\n        self.head = nn.Linear(\n            self.backbone.config.hidden_size * self.pooling.feat_mult, self.n_classes\n        )\n\n    def get_features(self, batch):\n        attention_mask = batch[\"attention_mask\"]\n        input_ids = batch[\"input_ids\"]\n\n        x = self.backbone(\n            input_ids=input_ids, attention_mask=attention_mask\n        ).last_hidden_state\n\n        x = self.pooling(x, attention_mask, input_ids, cfg=self.cfg)\n        x = torch.cat(x)\n\n        return x\n\n    def forward(self, batch):\n        idx = int(torch.where(batch[\"attention_mask\"] == 1)[1].max())\n        idx += 1\n        batch[\"attention_mask\"] = batch[\"attention_mask\"][:, :idx]\n        batch[\"input_ids\"] = batch[\"input_ids\"][:, :idx]\n\n        x = self.get_features(batch)\n        logits = self.head(x)\n\n        return {\"logits\": logits}","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:45:35.684592Z","iopub.execute_input":"2022-09-15T21:45:35.685013Z","iopub.status.idle":"2022-09-15T21:45:35.704429Z","shell.execute_reply.started":"2022-09-15T21:45:35.684980Z","shell.execute_reply":"2022-09-15T21:45:35.703147Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def run_predictions(cfg, grp_texts, bs=1):\n    ds = FeedbackDataset(grp_texts)\n    device = CFG.my_device\n    print(device)\n    model = FeedbackModel(CFG.cache_dir).to(device).eval()\n    d = torch.load(f\"../input/checkpoints/microsoft-deberta-v3-base_fold2_best.pth\", map_location=device)\n    \n    model.load_state_dict(collections.OrderedDict(d[\"model\"]), strict=False)\n\n    dl = DataLoader(ds, shuffle=False, batch_size=bs, num_workers=N_CORES)\n    #print(dl.size())\n    \n    with torch.inference_mode():\n        preds = []\n        for batch in dl:\n            batch = FeedbackDataset.batch_to_device(batch, device)\n            \n            if device == \"cuda\":\n              \n                with torch.cuda.amp.autocast():\n                  out = model(batch)\n                  preds.append(\n                      out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy()\n                  )\n            else:\n              \n                with torch.cpu.amp.autocast():\n                  out = model(batch)\n                  preds.append(\n                      out[\"logits\"].float().softmax(dim=1).detach().cpu().numpy()\n                  )\n\n    return np.concatenate(preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:48:45.870384Z","iopub.execute_input":"2022-09-15T21:48:45.870855Z","iopub.status.idle":"2022-09-15T21:48:45.884928Z","shell.execute_reply.started":"2022-09-15T21:48:45.870815Z","shell.execute_reply":"2022-09-15T21:48:45.883772Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#from huggingface_hub import notebook_login\n#notebook_login() #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","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pp = run_predictions(cfg, grp_texts=grp_texts, bs=16)\n\nprint(CFG.my_device)\npp = run_predictions(CFG, grp_texts=grp_texts, bs=16)","metadata":{"execution":{"iopub.status.busy":"2022-09-15T21:48:54.698394Z","iopub.execute_input":"2022-09-15T21:48:54.698840Z","iopub.status.idle":"2022-09-15T23:25:07.080698Z","shell.execute_reply.started":"2022-09-15T21:48:54.698803Z","shell.execute_reply":"2022-09-15T23:25:07.077831Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cpu\ncpu\n{'input_ids': tensor([     1,    262,   6410,   2274,    632,    330, 128019, 128019,    284,\n           266,   2274,    267,    466, 128019, 128019,   1180, 128019, 128019,\n          3022,    305,    598,      2,    647,  87839,    616,    724,   6410,\n          2274,    632,    330, 128019, 128019,    284,    266,   2274,    267,\n           466, 128019, 128019,   1180, 128019, 128019,   3022,    305,    598,\n           592,    262,   6410,   2274,    632,    330, 128019, 128019,    284,\n           266,   2274,    267,    466, 128019, 128019,   1180, 128019, 128019,\n          3022,    305,    598,    647,  27190,    616,    724,   6410,   2274,\n           632,    330, 128019, 128019,    284,    266,   2274,    267,    466,\n        128019, 128019,   1180, 128019, 128019,   3022,    305,    598,    592,\n             2,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_205/1459706457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrp_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_205/739920345.py\u001b[0m in \u001b[0;36mrun_predictions\u001b[0;34m(cfg, grp_texts, bs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                   preds.append(\n\u001b[1;32m     30\u001b[0m                       \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_205/554780675.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_205/554780675.py\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m         ).last_hidden_state\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_205/554780675.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attention_mask, input_ids, cfg)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mxxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n"],"ename":"NotImplementedError","evalue":"There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n","output_type":"error"}]},{"cell_type":"code","source":"# target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\ndf[\"cohesion\"] = pp[:, 0] \ndf[\"syntax\"] = pp[:, 1] \ndf[\"vocabulary\"] = pp[:, 2]\ndf[\"phraseology\"] = pp[:,3]\ndf[\"grammar\"] = pp[:,4]\ndf[\"conventions\"] = pp[:,5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_preds = pp.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\nlabel_cols = CFG.target_cols\noof_cols = []\nfor j, l in enumerate(label_cols):\n\n    df[f\"oof_{l}\"] = pp[:,j]\n    oof_cols.append(f\"oof_{l}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackStackerModel(nn.Module):\n    def __init__(self, n_features):\n        super(FeedbackStackerModel, self).__init__()\n\n        self.sizes = [256, 128, 64]\n\n        self.features = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(n_features, self.sizes[0])),\n            nn.PReLU(),\n            nn.Linear(self.sizes[0], self.sizes[1]),\n            nn.PReLU(),\n            nn.Linear(self.sizes[1], self.sizes[2]),\n            nn.PReLU(),\n        )\n        self.head = nn.Linear(self.sizes[-1], 3)\n\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x, y):\n        x = self.features(x)\n        x = self.head(x)\n\n        return {\"logits\": x}\n\n\nclass FeedbackStackerDataset(Dataset):\n    def __init__(self, df, mode):\n        self.df = df.copy().reset_index(drop=True)\n        self.mode = mode\n\n        self.feature_cols = oof_cols.copy()\n        self.label_cols = label_cols.copy()\n\n        df = self.df\n        '''\n        df[\"len\"] = df.groupby(\"text_id\")[f\"discourse_id\"].transform(\"count\") / 10\n        self.feature_cols.append(\"len\")\n        '''\n        df[\"len\"] = df.groupby(\"text_id\").transform(\"count\") / 10\n        self.feature_cols.append(\"len\")\n\n        for j, l in enumerate(label_cols):\n            df[f\"oof_{l}_mean\"] = df.groupby(\"text_id\")[f\"oof_{l}\"].transform(\"mean\")\n            self.feature_cols.append(f\"oof_{l}_mean\")\n            df[f\"oof_{l}_t_mean\"] = df.groupby([\"text_id\"])[\n                f\"oof_{l}\"\n            ].transform(\"mean\")\n            \n            '''\n            df[f\"oof_{l}_t_mean\"] = df.groupby([\"text_id\", \"discourse_type\"])[\n                f\"oof_{l}\"\n            ].transform(\"mean\")\n            '''\n            self.feature_cols.append(f\"oof_{l}_t_mean\")\n\n        self.num_features = len(self.feature_cols)\n\n        self.X = self.df[self.feature_cols].values\n        self.y = self.df[self.label_cols].values\n\n    def __getitem__(self, idx):\n        X = self.X[idx]\n        y = self.y[idx]\n\n        return torch.FloatTensor(X), torch.FloatTensor(y)\n\n    def __len__(self):\n        return self.df.shape[0]\n\n\nds = FeedbackStackerDataset(df.copy(), mode=\"val\")\n\n\ndef run_nn_stacker(df, BS=64):\n    ds = FeedbackStackerDataset(df.iloc[:].copy(), mode=\"test\")\n    #../input/checkpoints/microsoft-deberta-v3-base_fold2_best.pth\n    checkpoints = glob(f\"../input/checkpoints/*.pth\")\n    device = CFG.my_device\n\n    models = []\n    for checkpoint in checkpoints:\n        print(f\"running model {checkpoint}\")\n\n        model = FeedbackStackerModel(n_features=ds.num_features).to(device).eval()\n        model_weights = torch.load(checkpoint, map_location=device)\n\n        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)\n        models.append(model)\n\n    dl = DataLoader(ds, shuffle=False, batch_size=BS, num_workers=N_CORES)\n\n    with torch.no_grad():\n        preds = []\n        for batch in dl:\n            data = [x.to(\"cuda\") for x in batch]\n            inputs, target = data\n            p = []\n            for model in models:\n                out = model(inputs, target)\n                p.append(out[\"logits\"].float().softmax(dim=1))\n            preds.append(torch.mean(torch.stack(p), dim=0).detach().cpu().numpy())\n\n    preds = np.concatenate(preds, axis=0)\n\n    return preds\n\n\nnn_stacker_preds = run_nn_stacker(df, BS=256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_x(values):\n    return np.histogram(\n        np.clip(values, 0.001, 0.999), bins=3, density=True, range=(0, 1)\n    )[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nall_groups = []\n\ngb = df.groupby(\"text_id\", sort=False)\n\ndf[\"n_types\"] = gb[\"syntax\"].transform(lambda x: x.nunique())\ndf[\"mean_Ineffective\"] = gb[\"Ineffective\"].transform(\"mean\")\n\nfor name, group in gb:\n    class_name = \"Ineffective\"\n    for idx, val in enumerate(gen_x(group[class_name].values)):\n        group[f\"{class_name}_bin_{idx}\"] = val\n\n    all_groups.append(group)\n\ndf = pd.concat(all_groups).reset_index(drop=True)\n\ndisc_types_mapping = {\n    \"Lead\": 0,\n    \"Position\": 1,\n    \"Claim\": 2,\n    \"Evidence\": 3,\n    \"Counterclaim\": 4,\n    \"Rebuttal\": 5,\n    \"Concluding Statement\": 6,\n}\n\ndf[\"len_disc\"] = df.discourse_text.str.len()\ndf[\"discourse_type\"] = df[\"discourse_type\"].map(disc_types_mapping)\ndf[\"paragraph_cnt\"] = df.essay_text.map(lambda x: len(x.split(\"\\n\\n\")))\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use xgboost instead\n\nlgb_stacker_preds = []\n\nfor fold in range(5):\n    gbm = lightgbm.Booster(model_file=f\"../input/efficiency-prize-v2/lightgbm/model_fold_{fold}.txt\")\n    valid_pred = gbm.predict(\n        df[\n            [\n                \"discourse_type\",\n                \"Adequate\",\n                \"Effective\",\n                \"Ineffective\",\n                \"n_types\",\n                \"Ineffective_bin_0\",\n                \"Ineffective_bin_1\",\n                \"Ineffective_bin_2\",\n                \"mean_Ineffective\",\n                \"len_disc\",\n                \"paragraph_cnt\",\n            ]\n        ]\n    )\n    lgb_stacker_preds.append(valid_pred)\n    \nlgb_stacker_preds = np.array(lgb_stacker_preds).mean(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = [\n    orig_preds,\n    lgb_stacker_preds,\n    nn_stacker_preds,\n]\n\nall_preds = np.average(all_preds, axis=0, weights=[2, 1, 1 ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Adequate\"] = all_preds[:, 0]\ndf[\"Effective\"] = all_preds[:, 1]\ndf[\"Ineffective\"] = all_preds[:, 2]\n\ndf[[\"discourse_id\", \"Ineffective\", \"Adequate\", \"Effective\"]].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CALC_SCORE:\n    from sklearn.metrics import log_loss\n    \n    label_cols = [\"Adequate\", \"Effective\", \"Ineffective\"]\n    \n    y = np.zeros_like(all_preds)\n    \n    for ii, jj in enumerate([label_cols.index(x) for x in df[\"discourse_effectiveness\"].values]):\n        y[ii,jj] = 1\n        \n    print(log_loss(y, df[label_cols]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
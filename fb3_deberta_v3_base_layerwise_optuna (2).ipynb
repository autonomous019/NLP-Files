{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa5f4dbbbe784b6590d7de03d6a021a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c68918fa7734a00b02ffc047716690f",
              "IPY_MODEL_adae1054ccb340e6ba08c72ed6001c59",
              "IPY_MODEL_3df9d031164342ee84a7da12c328c34f"
            ],
            "layout": "IPY_MODEL_663e581d1df54f97b19e9a0afd7be684"
          }
        },
        "9c68918fa7734a00b02ffc047716690f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_304237e758e042199cb0295d599ea4ff",
            "placeholder": "​",
            "style": "IPY_MODEL_11096f962a384cdab4bc685717dcb787",
            "value": "100%"
          }
        },
        "adae1054ccb340e6ba08c72ed6001c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fc2a71fa8c4412abf23b2f2352cfe0c",
            "max": 3911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f57a35447aeb404bbd14cfa1b7046b18",
            "value": 3911
          }
        },
        "3df9d031164342ee84a7da12c328c34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f92bf4128e44399a2af550da465bda",
            "placeholder": "​",
            "style": "IPY_MODEL_f9cafbe9402b41b9808819100250a4e1",
            "value": " 3911/3911 [00:07&lt;00:00, 533.48it/s]"
          }
        },
        "663e581d1df54f97b19e9a0afd7be684": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "304237e758e042199cb0295d599ea4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11096f962a384cdab4bc685717dcb787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fc2a71fa8c4412abf23b2f2352cfe0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57a35447aeb404bbd14cfa1b7046b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04f92bf4128e44399a2af550da465bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9cafbe9402b41b9808819100250a4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "this code was based on FB3 / Deberta-v3-base baseline [train]\n",
        "# https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train\n",
        "\n",
        "- Deberta-v3-base starter code\n",
        "- pip wheels is [here](https://www.kaggle.com/code/yasufuminakama/fb3-pip-wheels)\n",
        "- Inference notebook is [here](https://www.kaggle.com/yasufuminakama/fb3-deberta-v3-base-baseline-inference)\n",
        "\n",
        "\n",
        "I thought I would share this notebook with the additions of Layer Wise Learning and Optuna Hyperparameter tuning since the original author of this notebook released it and saved me a great deal of time.  I am new to NLP and Kaggle competitions so I doubt I will be taking any gold medals with my work so might as well share with others that are also new and learning. I have tried to verify the code as accurate please comment or share corrections, bugs are always to be hunted down. "
      ],
      "metadata": {
        "papermill": {
          "duration": 0.012455,
          "end_time": "2022-08-31T07:01:57.321119",
          "exception": false,
          "start_time": "2022-08-31T07:01:57.308664",
          "status": "completed"
        },
        "tags": [],
        "id": "Jt1c2OFVpzph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports of Libraries and Modules"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011359,
          "end_time": "2022-08-31T07:01:57.342988",
          "exception": false,
          "start_time": "2022-08-31T07:01:57.331629",
          "status": "completed"
        },
        "tags": [],
        "id": "ZKwmr3zTpzpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################ NOTES ####################\n",
        "# this code was based on from FB3 / Deberta-v3-base baseline [train]\n",
        "# https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train\n",
        "#\n",
        "#\n",
        "#  - see ensemble learning, not yet implemented, see https://www.kaggle.com/code/gilfernandes/commonlit-pytorch-ensemble-large/notebook \n",
        "#  - following optuna implementation is based on https://github.com/gilfernandes/commonlit\n",
        "#  - original optunua notebook at https://github.com/gilfernandes/commonlit/blob/main/72_pytorch_transformers_deberta_optuna.ipynb\n",
        "#  - see article at https://signal.onepointltd.com/post/102h4el/modern-natural-language-processing-on-kaggle\n",
        "#  - guide to HF scheduler and differential learning rate https://www.kaggle.com/code/rhtsingh/guide-to-huggingface-schedulers-differential-lrs/notebook\n",
        "#  - learning rate schedulers https://www.kaggle.com/code/snnclsr/learning-rate-schedulers\n",
        "#  - optuna toy example https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.pyc\n",
        "#  - K-Folding, https://cran.r-project.org/web/packages/cvms/vignettes/picking_the_number_of_folds_for_cross-validation.html\n",
        "#  - deberta-v2 documentation: https://huggingface.co/transformers/v4.7.0/model_doc/deberta_v2.html\n",
        "#  - torch optimization documentation, to adjust activation and learning scheduler https://alband.github.io/doc_view/optim.html\n",
        "#  - HF, optimization docs, https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
        "#  - layerwise learning was based on https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403?sk=0fc1d1199174a065636c186e90342c90\n",
        "#  - layer wise learning was based on this roberta version, ported to deberta https://github.com/peggy1502/Data-Science-Articles/blob/main/train-roberta-advanced.ipynb\n",
        "#\n",
        "###########################################\n",
        "\n",
        "#TODO https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/350363\n",
        "'''\n",
        "this code was based on from FB3 / Deberta-v3-base baseline [train]\n",
        "https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import ast\n",
        "import sys\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import string\n",
        "import pickle\n",
        "import random\n",
        "import joblib\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
        "\n",
        "os.system('pip install iterative-stratification==0.1.7')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "os.system('python -m pip install tokenizers')\n",
        "os.system('python -m pip install transformers')\n",
        "import tokenizers\n",
        "import transformers\n",
        "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
        "print(f\"transformers.__version__: {transformers.__version__}\")\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n",
        "%env TOKENIZERS_PARALLELISM=true\n",
        "\n",
        "os.system('python -m pip install optuna')\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "papermill": {
          "duration": 57.587416,
          "end_time": "2022-08-31T07:03:04.064247",
          "exception": false,
          "start_time": "2022-08-31T07:02:06.476831",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:10:06.066712Z",
          "iopub.execute_input": "2022-09-27T03:10:06.067754Z",
          "iopub.status.idle": "2022-09-27T03:10:54.442826Z",
          "shell.execute_reply.started": "2022-09-27T03:10:06.067711Z",
          "shell.execute_reply": "2022-09-27T03:10:54.441741Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j89sN0vQpzpx",
        "outputId": "e27f826d-0628-4314-b5aa-3e6ada6f10d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizers.__version__: 0.12.1\n",
            "transformers.__version__: 4.22.2\n",
            "env: TOKENIZERS_PARALLELISM=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Directory settings\n",
        "# ====================================================\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025315,
          "end_time": "2022-08-31T07:01:57.378897",
          "exception": false,
          "start_time": "2022-08-31T07:01:57.353582",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:09:55.697852Z",
          "iopub.execute_input": "2022-09-27T03:09:55.698254Z",
          "iopub.status.idle": "2022-09-27T03:09:55.704551Z",
          "shell.execute_reply.started": "2022-09-27T03:09:55.698220Z",
          "shell.execute_reply": "2022-09-27T03:09:55.703289Z"
        },
        "trusted": true,
        "id": "vHxpcOVZpzpl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CFG"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.006569,
          "end_time": "2022-08-31T07:01:57.395826",
          "exception": false,
          "start_time": "2022-08-31T07:01:57.389257",
          "status": "completed"
        },
        "tags": [],
        "id": "9caYgVfxpzpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    wandb=False #set to True or False to use Wandb.ai for metrics\n",
        "    google_colab=True #set to true if working on google colab\n",
        "    competition='FB3'\n",
        "    _wandb_kernel='ell' #wandb.ai setting\n",
        "    debug=False\n",
        "    apex=True\n",
        "    print_freq=20\n",
        "    num_workers=4\n",
        "    model=\"microsoft/deberta-v3-base\"\n",
        "    gradient_checkpointing=True\n",
        "    scheduler='cosine' # ['linear', 'cosine'] #deprecated for optuna, scheduler set in objective()\n",
        "    batch_scheduler=True\n",
        "    num_cycles=0.5\n",
        "    encoder_lr=1.5e-5 #1.5 @.448 #deprecated for optuna, set in objective()\n",
        "    decoder_lr=1.5e-5 #deprecated\n",
        "    min_lr=1e-6 \n",
        "    eps=1e-6\n",
        "    betas=(0.9, 0.999) #activaion agruments for optimizer, lookup in docs\n",
        "    batch_size=2 #originally set to 8\n",
        "    max_len=512 #originally 512\n",
        "    weight_decay=0.01\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1000\n",
        "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    seed=1003  #testing 3, 1003\n",
        "    num_warmup_steps=1 #originally set to 0\n",
        "    epochs=4 # the internet says this should be 3x num of classes (6) target_cols = 18\n",
        "    n_fold=4 #originally set to 4 then 5\n",
        "    #trn_fold = [0,1]\n",
        "    trn_fold=[0, 1, 2, 3]\n",
        "    n_trials=5\n",
        "    train=True\n",
        "    \n",
        "    \n",
        "if CFG.debug:\n",
        "    CFG.epochs = 1 # 2\n",
        "    CFG.trn_fold = [0]"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.015868,
          "end_time": "2022-08-31T07:01:57.417077",
          "exception": false,
          "start_time": "2022-08-31T07:01:57.401209",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:09:55.711004Z",
          "iopub.execute_input": "2022-09-27T03:09:55.711311Z",
          "iopub.status.idle": "2022-09-27T03:09:55.721438Z",
          "shell.execute_reply.started": "2022-09-27T03:09:55.711285Z",
          "shell.execute_reply": "2022-09-27T03:09:55.720460Z"
        },
        "trusted": true,
        "id": "vdsob2-lpzpu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# wandb\n",
        "# ====================================================\n",
        "if CFG.wandb:\n",
        "    os.system('python -m pip install wandb')\n",
        "    import wandb\n",
        "\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        user_secrets = UserSecretsClient()\n",
        "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
        "        wandb.login(key=secret_value_0)\n",
        "        anony = None\n",
        "    except:\n",
        "        anony = \"must\"\n",
        "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
        "\n",
        "\n",
        "    def class2dict(f):\n",
        "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
        "\n",
        "    run = wandb.init(project='ell', \n",
        "                     name=CFG.model,\n",
        "                     config=class2dict(CFG),\n",
        "                     group=CFG.model,\n",
        "                     job_type=\"train\",\n",
        "                     anonymous=anony)"
      ],
      "metadata": {
        "papermill": {
          "duration": 9.02584,
          "end_time": "2022-08-31T07:02:06.448566",
          "exception": false,
          "start_time": "2022-08-31T07:01:57.422726",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:09:55.735985Z",
          "iopub.execute_input": "2022-09-27T03:09:55.736264Z",
          "iopub.status.idle": "2022-09-27T03:10:06.061872Z",
          "shell.execute_reply.started": "2022-09-27T03:09:55.736238Z",
          "shell.execute_reply": "2022-09-27T03:10:06.060887Z"
        },
        "trusted": true,
        "id": "zXO84eYFpzpv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008876,
          "end_time": "2022-08-31T07:02:06.467728",
          "exception": false,
          "start_time": "2022-08-31T07:02:06.458852",
          "status": "completed"
        },
        "tags": [],
        "id": "fyRJe-PPpzpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Directory settings\n",
        "# ====================================================\n",
        "import os\n",
        "\n",
        "if CFG.google_colab:\n",
        "  # Import from GoogleDrive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  os.chdir(\"//content/gdrive/MyDrive/feedback-prize-english-language-learning\")\n",
        "\n",
        "  save_dir = \"/content/gdrive/My Drive/feedback-prize-english-language-learning/submission\"\n",
        "  logs_dir = \"/content/gdrive/My Drive/feedback-prize-english-language-learning/logs\"\n",
        "  data_dir = \"/content/gdrive/My Drive/feedback-prize-english-language-learning\"\n",
        "  model_dir = \"/content/gdrive/My Drive/ml_models/model\"\n",
        "  import os.path\n",
        "  from os import path\n",
        "\n",
        "  model_folder = CFG.model.replace('/', '-')\n",
        "  if path.exists(data_dir + \"/\" + \"trained_\" + model_folder) == False:\n",
        "    os.mkdir(data_dir + \"/\" + \"trained_\" + model_folder)\n",
        "  save_model_dir = data_dir + \"/\" + \"trained_\" + model_folder\n",
        "\n",
        "if(CFG.google_colab):\n",
        "  CFG.OUTPUT_DIR = save_model_dir\n",
        "  CFG.DATA_DIR = data_dir\n",
        "else:\n",
        "  CFG.OUTPUT_DIR = './' #for running on kaggle\n",
        "  CFG.DATA_DIR = '../input/feedback-prize-english-language-learning'\n",
        "\n",
        "OUTPUT_DIR = CFG.OUTPUT_DIR\n",
        "DATA_DIR = CFG.DATA_DIR\n",
        "\n",
        "print(OUTPUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdFyNBVdqlQP",
        "outputId": "83545547-7bf1-4b26-d1a5-f69dd12e979d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/feedback-prize-english-language-learning/trained_microsoft-deberta-v3-base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007998,
          "end_time": "2022-08-31T07:03:04.079768",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.07177",
          "status": "completed"
        },
        "tags": [],
        "id": "1ki1dCDupzpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Utils\n",
        "# ====================================================\n",
        "def MCRMSE(y_trues, y_preds):\n",
        "    scores = []\n",
        "    idxes = y_trues.shape[1]\n",
        "    for i in range(idxes):\n",
        "        y_true = y_trues[:,i]\n",
        "        y_pred = y_preds[:,i]\n",
        "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
        "        scores.append(score)\n",
        "    mcrmse_score = np.mean(scores)\n",
        "    return mcrmse_score, scores\n",
        "\n",
        "def SWISH(x):\n",
        "    s = x* (1/(1+np.exp(-x)))\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_score(y_trues, y_preds):\n",
        "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
        "    return mcrmse_score, scores\n",
        "\n",
        "\n",
        "def get_logger(filename=OUTPUT_DIR+'train'):\n",
        "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = get_logger()\n",
        "\n",
        "\n",
        "def seed_everything(seed=CFG.seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed=CFG.seed)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.024132,
          "end_time": "2022-08-31T07:03:04.111108",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.086976",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:10:54.444269Z",
          "iopub.execute_input": "2022-09-27T03:10:54.444931Z",
          "iopub.status.idle": "2022-09-27T03:10:54.459504Z",
          "shell.execute_reply.started": "2022-09-27T03:10:54.444892Z",
          "shell.execute_reply": "2022-09-27T03:10:54.458343Z"
        },
        "trusted": true,
        "id": "PDR2ui4upzpy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.012589,
          "end_time": "2022-08-31T07:03:04.13341",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.120821",
          "status": "completed"
        },
        "tags": [],
        "id": "84lQv-g7pzpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Data Loading\n",
        "# ====================================================\n",
        "\n",
        "\n",
        "train = pd.read_csv(DATA_DIR + '/train.csv')\n",
        "test = pd.read_csv(DATA_DIR + '/test.csv')\n",
        "submission = pd.read_csv(DATA_DIR + '/sample_submission.csv')\n",
        "\n",
        "train_df = train\n",
        "val_df = test\n",
        "\n",
        "print(f\"train.shape: {train.shape}\")\n",
        "display(train.head())\n",
        "print(f\"test.shape: {test.shape}\")\n",
        "display(test.head())\n",
        "print(f\"submission.shape: {submission.shape}\")\n",
        "display(submission.head())"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.242687,
          "end_time": "2022-08-31T07:03:04.383434",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.140747",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:10:54.462597Z",
          "iopub.execute_input": "2022-09-27T03:10:54.463026Z",
          "iopub.status.idle": "2022-09-27T03:10:54.596331Z",
          "shell.execute_reply.started": "2022-09-27T03:10:54.462987Z",
          "shell.execute_reply": "2022-09-27T03:10:54.595470Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "aIwa316-pzp0",
        "outputId": "52efab7b-94ec-40df-dfec-c831f77ca249"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.shape: (3911, 8)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67127c62-5600-4a20-bb1e-2baf2b88deba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67127c62-5600-4a20-bb1e-2baf2b88deba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67127c62-5600-4a20-bb1e-2baf2b88deba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67127c62-5600-4a20-bb1e-2baf2b88deba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.shape: (3, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        text_id                                          full_text\n",
              "0  0000C359D63E  when a person has no experience on a job their...\n",
              "1  000BAD50D026  Do you think students would benefit from being...\n",
              "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9349d336-9aab-4897-afd7-77b3983c8e28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>when a person has no experience on a job their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>Do you think students would benefit from being...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9349d336-9aab-4897-afd7-77b3983c8e28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9349d336-9aab-4897-afd7-77b3983c8e28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9349d336-9aab-4897-afd7-77b3983c8e28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.shape: (3, 7)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
              "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n",
              "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-187cdbd5-8c49-491c-aa6a-69cc5704d294\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-187cdbd5-8c49-491c-aa6a-69cc5704d294')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-187cdbd5-8c49-491c-aa6a-69cc5704d294 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-187cdbd5-8c49-491c-aa6a-69cc5704d294');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CV split"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008293,
          "end_time": "2022-08-31T07:03:04.40102",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.392727",
          "status": "completed"
        },
        "tags": [],
        "id": "hWnF5fS3pzp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# CV split\n",
        "# ====================================================\n",
        "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
        "    train.loc[val_index, 'fold'] = int(n)\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "display(train.groupby('fold').size())"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.148391,
          "end_time": "2022-08-31T07:03:04.558882",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.410491",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:10:54.597877Z",
          "iopub.execute_input": "2022-09-27T03:10:54.598502Z",
          "iopub.status.idle": "2022-09-27T03:10:54.749751Z",
          "shell.execute_reply.started": "2022-09-27T03:10:54.598463Z",
          "shell.execute_reply": "2022-09-27T03:10:54.748880Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "QPOM7gfVpzqA",
        "outputId": "93613d34-9361-49d1-916c-1a7bc135500d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fold\n",
              "0    978\n",
              "1    977\n",
              "2    978\n",
              "3    978\n",
              "dtype: int64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if CFG.debug:\n",
        "    display(train.groupby('fold').size())\n",
        "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
        "    display(train.groupby('fold').size())"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.019618,
          "end_time": "2022-08-31T07:03:04.586414",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.566796",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:10:54.751940Z",
          "iopub.execute_input": "2022-09-27T03:10:54.753017Z",
          "iopub.status.idle": "2022-09-27T03:10:54.761507Z",
          "shell.execute_reply.started": "2022-09-27T03:10:54.752978Z",
          "shell.execute_reply": "2022-09-27T03:10:54.759089Z"
        },
        "trusted": true,
        "id": "GNIGX-BHpzqB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tokenizer"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007561,
          "end_time": "2022-08-31T07:03:04.604916",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.597355",
          "status": "completed"
        },
        "tags": [],
        "id": "uzizpox9pzqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# tokenizer\n",
        "# ====================================================\n",
        "os.system('python -m pip install sentencepiece')\n",
        "import sentencepiece #deberta specific may not be needed for other models\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR+'/tokenizer/')\n",
        "CFG.tokenizer = tokenizer"
      ],
      "metadata": {
        "papermill": {
          "duration": 7.351568,
          "end_time": "2022-08-31T07:03:11.964298",
          "exception": false,
          "start_time": "2022-08-31T07:03:04.61273",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:10:54.763314Z",
          "iopub.execute_input": "2022-09-27T03:10:54.766123Z",
          "iopub.status.idle": "2022-09-27T03:11:00.691782Z",
          "shell.execute_reply.started": "2022-09-27T03:10:54.766080Z",
          "shell.execute_reply": "2022-09-27T03:11:00.690719Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOH7k7_ApzqE",
        "outputId": "351ad2eb-e0db-4928-b1d8-9bd280426934"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008127,
          "end_time": "2022-08-31T07:03:11.985369",
          "exception": false,
          "start_time": "2022-08-31T07:03:11.977242",
          "status": "completed"
        },
        "tags": [],
        "id": "3t6AH9uEpzqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Define max_len\n",
        "# ====================================================\n",
        "lengths = []\n",
        "tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
        "for text in tk0:\n",
        "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
        "    lengths.append(length)\n",
        "CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
        "LOGGER.info(f\"max_len: {CFG.max_len}\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.893032,
          "end_time": "2022-08-31T07:03:17.886504",
          "exception": false,
          "start_time": "2022-08-31T07:03:11.993472",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:00.694278Z",
          "iopub.execute_input": "2022-09-27T03:11:00.695383Z",
          "iopub.status.idle": "2022-09-27T03:11:07.045832Z",
          "shell.execute_reply.started": "2022-09-27T03:11:00.695346Z",
          "shell.execute_reply": "2022-09-27T03:11:07.044835Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "fa5f4dbbbe784b6590d7de03d6a021a3",
            "9c68918fa7734a00b02ffc047716690f",
            "adae1054ccb340e6ba08c72ed6001c59",
            "3df9d031164342ee84a7da12c328c34f",
            "663e581d1df54f97b19e9a0afd7be684",
            "304237e758e042199cb0295d599ea4ff",
            "11096f962a384cdab4bc685717dcb787",
            "8fc2a71fa8c4412abf23b2f2352cfe0c",
            "f57a35447aeb404bbd14cfa1b7046b18",
            "04f92bf4128e44399a2af550da465bda",
            "f9cafbe9402b41b9808819100250a4e1"
          ]
        },
        "id": "qp1GdULTpzqZ",
        "outputId": "2735121d-e89b-4630-d3d6-a196c0cf1772"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3911 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa5f4dbbbe784b6590d7de03d6a021a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_len: 1429\n",
            "INFO:__main__:max_len: 1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Dataset\n",
        "# ====================================================\n",
        "def prepare_input(cfg, text):\n",
        "    inputs = cfg.tokenizer.encode_plus(\n",
        "        text, \n",
        "        return_tensors=None, \n",
        "        add_special_tokens=True, \n",
        "        max_length=CFG.max_len,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, cfg, df):\n",
        "        self.cfg = cfg\n",
        "        self.texts = df['full_text'].values\n",
        "        self.labels = df[cfg.target_cols].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        inputs = prepare_input(self.cfg, self.texts[item])\n",
        "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
        "        return inputs, label\n",
        "    \n",
        "\n",
        "def collate(inputs):\n",
        "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = inputs[k][:,:mask_len]\n",
        "    return inputs"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.020447,
          "end_time": "2022-08-31T07:03:17.916566",
          "exception": false,
          "start_time": "2022-08-31T07:03:17.896119",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.047782Z",
          "iopub.execute_input": "2022-09-27T03:11:07.048526Z",
          "iopub.status.idle": "2022-09-27T03:11:07.060315Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.048487Z",
          "shell.execute_reply": "2022-09-27T03:11:07.059286Z"
        },
        "trusted": true,
        "id": "PyEArWMqpzqa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008073,
          "end_time": "2022-08-31T07:03:17.933189",
          "exception": false,
          "start_time": "2022-08-31T07:03:17.925116",
          "status": "completed"
        },
        "tags": [],
        "id": "pPA2ra05pzqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Model\n",
        "# ====================================================\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanPooling, self).__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        return mean_embeddings\n",
        "    \n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        if config_path is None:\n",
        "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
        "            self.config.hidden_dropout = 0.\n",
        "            self.config.hidden_dropout_prob = 0.\n",
        "            self.config.attention_dropout = 0.\n",
        "            self.config.attention_probs_dropout_prob = 0.\n",
        "            LOGGER.info(self.config)\n",
        "        else:\n",
        "            self.config = torch.load(config_path)\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
        "        else:\n",
        "            self.model = AutoModel(self.config)\n",
        "        if self.cfg.gradient_checkpointing:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "        self.pool = MeanPooling()\n",
        "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
        "        self._init_weights(self.fc)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        \n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        last_hidden_states = outputs[0]\n",
        "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
        "        return feature\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature = self.feature(inputs)\n",
        "        output = self.fc(feature)\n",
        "        return output"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.033105,
          "end_time": "2022-08-31T07:03:17.97447",
          "exception": false,
          "start_time": "2022-08-31T07:03:17.941365",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.064362Z",
          "iopub.execute_input": "2022-09-27T03:11:07.064692Z",
          "iopub.status.idle": "2022-09-27T03:11:07.081197Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.064666Z",
          "shell.execute_reply": "2022-09-27T03:11:07.080235Z"
        },
        "trusted": true,
        "id": "lSKFbpmIpzqc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008106,
          "end_time": "2022-08-31T07:03:17.993861",
          "exception": false,
          "start_time": "2022-08-31T07:03:17.985755",
          "status": "completed"
        },
        "tags": [],
        "id": "sAckWu12pzqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Loss\n",
        "# ====================================================\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, reduction='mean', eps=1e-9):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        self.reduction = reduction\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
        "        if self.reduction == 'none':\n",
        "            loss = loss\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        return loss"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.021697,
          "end_time": "2022-08-31T07:03:18.02376",
          "exception": false,
          "start_time": "2022-08-31T07:03:18.002063",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.082761Z",
          "iopub.execute_input": "2022-09-27T03:11:07.083188Z",
          "iopub.status.idle": "2022-09-27T03:11:07.099778Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.083133Z",
          "shell.execute_reply": "2022-09-27T03:11:07.098580Z"
        },
        "trusted": true,
        "id": "fc3-ETYvpzqe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layerwise Learning**\n",
        "\n",
        "layer wise learning was based on this roberta version, ported to deberta https://github.com/peggy1502/Data-Science-Articles/blob/main/train-roberta-advanced.ipynb\n",
        "\n",
        "if you use another model you will need to make adjustments based on the model's parameters, model.named_parameters()\n",
        "\n",
        "There are two layerwise learning functions, one for grouped and one for setting the LR for each individual layer, rather then grouping the layers together. The functions return an optimizer, in this case AdamW which is set called in the objective() function. "
      ],
      "metadata": {
        "id": "bafzIrWppzqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deberta_base_AdamW_grouped_LLRD(model, lr, debug=False):\n",
        "        \n",
        "    opt_parameters = [] # To be passed to the optimizer (only parameters of the layers you want to update).\n",
        "    debug_param_groups = []\n",
        "    named_parameters = list(model.named_parameters()) \n",
        "    print(\"model parameters in grouped llrd func\")\n",
        "    print(model.parameters())\n",
        "    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n",
        "    # for bias and LayerNorm.weight layers.\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    set_2 = [\"layer.4\", \"layer.5\", \"layer.6\", \"layer.7\"]\n",
        "    set_3 = [\"layer.8\", \"layer.9\", \"layer.10\", \"layer.11\"]\n",
        "    #init_lr = 1e-6\n",
        "    init_lr = lr\n",
        "    \n",
        "    for i, (name, params) in enumerate(named_parameters):  \n",
        "        \n",
        "        weight_decay = 0.0 if any(p in name for p in no_decay) else 0.01\n",
        "        \n",
        "        if name.startswith(\"transformer_model.deberta.embeddings\") or name.startswith(\"transformer_model.deberta.encoder\"):            \n",
        "            # For first set, set lr to 1e-6 (i.e. 0.000001)\n",
        "            lr = init_lr       \n",
        "            \n",
        "            # For set_2, increase lr to 0.00000175\n",
        "            lr = init_lr * 1.75 if any(p in name for p in set_2) else lr\n",
        "            \n",
        "            # For set_3, increase lr to 0.0000035 \n",
        "            lr = init_lr * 3.5 if any(p in name for p in set_3) else lr\n",
        "            \n",
        "            opt_parameters.append({\"params\": params,\n",
        "                                   \"weight_decay\": weight_decay,\n",
        "                                   \"lr\": lr})  \n",
        "            \n",
        "        # For regressor and pooler, set lr to 0.0000036 (slightly higher than the top layer). \n",
        "        #transformer_model.pooler.dense.weight               \n",
        "        elif name.startswith(\"regressor\") or name.startswith(\"transformer_model.pooler\"):               \n",
        "            lr = init_lr * 3.6 \n",
        "            \n",
        "            opt_parameters.append({\"params\": params,\n",
        "                                   \"weight_decay\": weight_decay,\n",
        "                                   \"lr\": lr}) \n",
        "        else:\n",
        "            lr = init_lr \n",
        "            \n",
        "            opt_parameters.append({\"params\": params,\n",
        "                                   \"weight_decay\": weight_decay,\n",
        "                                   \"lr\": lr})\n",
        "            \n",
        "         \n",
        "\n",
        "        debug_param_groups.append(f\"{i} {name}\")\n",
        "    \n",
        "    if debug: \n",
        "        for g in range(len(debug_param_groups)): print(debug_param_groups[g]) \n",
        "\n",
        "    return transformers.AdamW(opt_parameters, lr=init_lr), debug_param_groups  #returns a list opt_parameters\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.104901Z",
          "iopub.execute_input": "2022-09-27T03:11:07.105336Z",
          "iopub.status.idle": "2022-09-27T03:11:07.123948Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.105298Z",
          "shell.execute_reply": "2022-09-27T03:11:07.122688Z"
        },
        "trusted": true,
        "id": "KcGVUaKApzqg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deberta_base_AdamW_LLRD(model, lr, debug=False):\n",
        "    #optimal learning rates, https://www.jeremyjordan.me/nn-learning-rate/\n",
        "\n",
        "    opt_parameters = [] # To be passed to the optimizer (only parameters of the layers you want to update).\n",
        "    named_parameters = list(model.named_parameters()) \n",
        "    debug_param_groups = []\n",
        "    \n",
        "    #print(named_parameters)\n",
        "    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n",
        "    # for bias and LayerNorm.weight layers.\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    init_lr = lr \n",
        "    head_lr = lr #needs to be a bit higher then init_lr, i.e. init_lr 3.5e5 at head 3.6e5\n",
        "    \n",
        "    \n",
        "    # === Pooler and regressor ======================================================  \n",
        "    \n",
        "    params_0 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n) \n",
        "                and any(nd in n for nd in no_decay)]\n",
        "    params_1 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n",
        "                and not any(nd in n for nd in no_decay)]\n",
        "    \n",
        "    head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}    \n",
        "    opt_parameters.append(head_params)\n",
        "    debug_param_groups.append(f\"head_params\")\n",
        "    \n",
        "    head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": 0.01}    \n",
        "    opt_parameters.append(head_params)\n",
        "    debug_param_groups.append(f\"head_params\")\n",
        "            \n",
        "    \n",
        "    # === 12 Hidden layers ==========================================================\n",
        "    \n",
        "    for layer in range(11,-1,-1):\n",
        "        \n",
        "        params_0 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
        "                    and any(nd in n for nd in no_decay)]\n",
        "        params_1 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
        "                    and not any(nd in n for nd in no_decay)]\n",
        "        \n",
        "        layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
        "        opt_parameters.append(layer_params)   \n",
        "        debug_param_groups.append(f\"layer.{layer}\")\n",
        "                    \n",
        "        layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n",
        "        opt_parameters.append(layer_params)\n",
        "        debug_param_groups.append(f\"layer.{layer}\")       \n",
        "        \n",
        "        lr *= 0.9 \n",
        "    \n",
        "    # === Embeddings layer ==========================================================\n",
        "    \n",
        "    params_0 = [p for n,p in named_parameters if \"embeddings\" in n \n",
        "                and any(nd in n for nd in no_decay)]\n",
        "    params_1 = [p for n,p in named_parameters if \"embeddings\" in n\n",
        "                and not any(nd in n for nd in no_decay)]\n",
        "    \n",
        "    embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n",
        "    opt_parameters.append(embed_params)\n",
        "    debug_param_groups.append(f\"embed_params\")\n",
        "    \n",
        "    embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n",
        "    opt_parameters.append(embed_params)\n",
        "    debug_param_groups.append(f\"embed_params\")\n",
        "    \n",
        "    if debug: \n",
        "        for g in range(len(debug_param_groups)): print(g, debug_param_groups[g]) \n",
        "\n",
        "    return transformers.AdamW(opt_parameters, lr=init_lr), debug_param_groups"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.125464Z",
          "iopub.execute_input": "2022-09-27T03:11:07.126095Z",
          "iopub.status.idle": "2022-09-27T03:11:07.150999Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.126061Z",
          "shell.execute_reply": "2022-09-27T03:11:07.148405Z"
        },
        "trusted": true,
        "id": "QNgRC0Ifpzqh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_lr_by_layers(optimizer, grouped_LLRD=True):    \n",
        "    lr = []\n",
        "    if grouped_LLRD:\n",
        "        lr.append(optimizer.param_groups[0][\"lr\"])   # embeddings\n",
        "        lr.append(optimizer.param_groups[3][\"lr\"])   # layer0\n",
        "        lr.append(optimizer.param_groups[19][\"lr\"])  # layer1\n",
        "        lr.append(optimizer.param_groups[35][\"lr\"])  # layer2\n",
        "        lr.append(optimizer.param_groups[51][\"lr\"])  # layer3\n",
        "        lr.append(optimizer.param_groups[67][\"lr\"])  # layer4\n",
        "        lr.append(optimizer.param_groups[83][\"lr\"])  # layer5\n",
        "        lr.append(optimizer.param_groups[99][\"lr\"]) # layer6\n",
        "        lr.append(optimizer.param_groups[115][\"lr\"]) # layer7\n",
        "        lr.append(optimizer.param_groups[131][\"lr\"]) # layer8\n",
        "        lr.append(optimizer.param_groups[147][\"lr\"]) # layer9\n",
        "        lr.append(optimizer.param_groups[163][\"lr\"]) # layer10\n",
        "        lr.append(optimizer.param_groups[179][\"lr\"]) # layer11\n",
        "        lr.append(optimizer.param_groups[198][\"lr\"]) # pooler\n",
        "        lr.append(optimizer.param_groups[206][\"lr\"]) # regressor \n",
        "    \n",
        "    else:\n",
        "\n",
        "        lr.append(optimizer.param_groups[26][\"lr\"]) # embeddings\n",
        "        lr.append(optimizer.param_groups[24][\"lr\"]) # layer0\n",
        "        lr.append(optimizer.param_groups[22][\"lr\"]) # layer1\n",
        "        lr.append(optimizer.param_groups[20][\"lr\"]) # layer2\n",
        "        lr.append(optimizer.param_groups[18][\"lr\"]) # layer3\n",
        "        lr.append(optimizer.param_groups[16][\"lr\"]) # layer4\n",
        "        lr.append(optimizer.param_groups[14][\"lr\"]) # layer5\n",
        "        lr.append(optimizer.param_groups[12][\"lr\"]) # layer6\n",
        "        lr.append(optimizer.param_groups[10][\"lr\"]) # layer7\n",
        "        lr.append(optimizer.param_groups[8][\"lr\"])  # layer8\n",
        "        lr.append(optimizer.param_groups[6][\"lr\"])  # layer9\n",
        "        lr.append(optimizer.param_groups[4][\"lr\"])  # layer10\n",
        "        lr.append(optimizer.param_groups[2][\"lr\"])  # layer11\n",
        "        lr.append(optimizer.param_groups[0][\"lr\"])  # pooler\n",
        "        lr.append(optimizer.param_groups[0][\"lr\"])  # regressor \n",
        "    return lr \n",
        "'''\n",
        "this function was used to write ouput of paremeters to a csv file, not implemented in this notebook\n",
        "\n",
        "\n",
        "lr_list = []\n",
        "lr_list2 = []\n",
        "#lr_list.append(optimizer.param_groups[0][\"lr\"])\n",
        "optimizer, debug = deberta_base_AdamW_LLRD(sample_model) # for per layer lr\n",
        "optimizer2, debug2 = deberta_base_AdamW_grouped_LLRD(sample_model) # for grouped lr\n",
        "print(\"optimizer type\")\n",
        "print(type(optimizer))\n",
        "#print(\"debug\")\n",
        "#print(debug)\n",
        "#collect_lr_by_layers(optimizer, grouped_LLRD=True)\n",
        "\n",
        "\n",
        "#lr_list2.append(collect_lr_by_layers(optimizer, grouped_LLRD=True))\n",
        "lr_list2 = collect_lr_by_layers(optimizer, grouped_LLRD=False)\n",
        "print(len(lr_list))\n",
        "print(len(lr_list2))\n",
        "print(lr_list2)\n",
        "\n",
        "lr_list2 = collect_lr_by_layers(optimizer2, grouped_LLRD=True)\n",
        "print(len(lr_list))\n",
        "print(len(lr_list2))\n",
        "print(lr_list2)\n",
        "'''        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.152882Z",
          "iopub.execute_input": "2022-09-27T03:11:07.153891Z",
          "iopub.status.idle": "2022-09-27T03:11:07.185776Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.153855Z",
          "shell.execute_reply": "2022-09-27T03:11:07.184820Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ikC4t43_pzqi",
        "outputId": "3f309775-3550-42d2-a202-7f3210e6af4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nthis function was used to write ouput of paremeters to a csv file, not implemented in this notebook\\n\\n\\nlr_list = []\\nlr_list2 = []\\n#lr_list.append(optimizer.param_groups[0][\"lr\"])\\noptimizer, debug = deberta_base_AdamW_LLRD(sample_model) # for per layer lr\\noptimizer2, debug2 = deberta_base_AdamW_grouped_LLRD(sample_model) # for grouped lr\\nprint(\"optimizer type\")\\nprint(type(optimizer))\\n#print(\"debug\")\\n#print(debug)\\n#collect_lr_by_layers(optimizer, grouped_LLRD=True)\\n\\n\\n#lr_list2.append(collect_lr_by_layers(optimizer, grouped_LLRD=True))\\nlr_list2 = collect_lr_by_layers(optimizer, grouped_LLRD=False)\\nprint(len(lr_list))\\nprint(len(lr_list2))\\nprint(lr_list2)\\n\\nlr_list2 = collect_lr_by_layers(optimizer2, grouped_LLRD=True)\\nprint(len(lr_list))\\nprint(len(lr_list2))\\nprint(lr_list2)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpler functions"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008452,
          "end_time": "2022-08-31T07:03:18.041557",
          "exception": false,
          "start_time": "2022-08-31T07:03:18.033105",
          "status": "completed"
        },
        "tags": [],
        "id": "LLM4CTzxpzqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
        "    print(\"TRAIN_FN()\")\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
        "    losses = AverageMeter()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    for step, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = collate(inputs)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
        "            y_preds = model(inputs)\n",
        "            loss = criterion(y_preds, labels)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        scaler.scale(loss).backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            if CFG.batch_scheduler:\n",
        "                scheduler.step()\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  'LR: {lr:.8f}  '\n",
        "                  .format(epoch+1, step, len(train_loader), \n",
        "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
        "                          loss=losses,\n",
        "                          grad_norm=grad_norm,\n",
        "                          lr=scheduler.get_lr()[0]))\n",
        "        if CFG.wandb:\n",
        "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
        "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    print(\"valid_FN()\")\n",
        "    losses = AverageMeter()\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    start = end = time.time()\n",
        "    for step, (inputs, labels) in enumerate(valid_loader):\n",
        "        inputs = collate(inputs)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        with torch.no_grad():\n",
        "            y_preds = model(inputs)\n",
        "            loss = criterion(y_preds, labels)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        preds.append(y_preds.to('cpu').numpy())\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(step, len(valid_loader),\n",
        "                          loss=losses,\n",
        "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
        "    predictions = np.concatenate(preds)\n",
        "    return losses.avg, predictions"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.030759,
          "end_time": "2022-08-31T07:03:18.08056",
          "exception": false,
          "start_time": "2022-08-31T07:03:18.049801",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.189312Z",
          "iopub.execute_input": "2022-09-27T03:11:07.189581Z",
          "iopub.status.idle": "2022-09-27T03:11:07.214203Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.189556Z",
          "shell.execute_reply": "2022-09-27T03:11:07.213074Z"
        },
        "trusted": true,
        "id": "TPSqC9hWpzqk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train loop"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.0081,
          "end_time": "2022-08-31T07:03:18.100232",
          "exception": false,
          "start_time": "2022-08-31T07:03:18.092132",
          "status": "completed"
        },
        "tags": [],
        "id": "HX_vbegnpzqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# train loop\n",
        "# ====================================================\n",
        "def train_loop(folds, fold, model, optimizer, scheduler, scaler, batch_size):\n",
        "    \n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # loader\n",
        "    # ====================================================\n",
        "    \n",
        "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
        "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
        "    valid_labels = valid_folds[CFG.target_cols].values\n",
        "    \n",
        "    train_dataset = TrainDataset(CFG, train_folds)\n",
        "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=batch_size * 2,\n",
        "                              shuffle=False,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "\n",
        "    # ====================================================\n",
        "    # model & optimizer\n",
        "    # ====================================================\n",
        "    #model = CustomModel(CFG, config_path=None, pretrained=True)\n",
        "    torch.save(model.config, OUTPUT_DIR+'/config.pt')\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # ====================================================\n",
        "    # loop\n",
        "    # ====================================================\n",
        "    criterion = nn.SmoothL1Loss(reduction='mean', beta = 100) # beta = 0.025 .450 RMSELoss(reduction=\"mean\") #added beta argument not in original notebook\n",
        "    \n",
        "    best_score = np.inf\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train\n",
        "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
        "        \n",
        "        # scoring\n",
        "        score, scores = get_score(valid_labels, predictions)\n",
        "        print(\"Score\")\n",
        "        print(score)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
        "        if CFG.wandb:\n",
        "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
        "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
        "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
        "                       f\"[fold{fold}] score\": score})\n",
        "        \n",
        "        if best_score > score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "            torch.save({'model': model.state_dict(),\n",
        "                        'predictions': predictions},\n",
        "                        OUTPUT_DIR+f\"/{CFG.model.replace('/', '-')}_fold{fold}_best.pt\")\n",
        "                        \n",
        "\n",
        "    predictions = torch.load(OUTPUT_DIR+f\"/{CFG.model.replace('/', '-')}_fold{fold}_best.pt\", \n",
        "                             map_location=torch.device('cpu'))['predictions']\n",
        "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return valid_folds, best_score"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.033332,
          "end_time": "2022-08-31T07:03:18.141812",
          "exception": false,
          "start_time": "2022-08-31T07:03:18.10848",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-09-27T03:11:07.215745Z",
          "iopub.execute_input": "2022-09-27T03:11:07.217030Z",
          "iopub.status.idle": "2022-09-27T03:11:07.237301Z",
          "shell.execute_reply.started": "2022-09-27T03:11:07.216991Z",
          "shell.execute_reply": "2022-09-27T03:11:07.236079Z"
        },
        "trusted": true,
        "id": "85wHz4-Tpzqu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna: Tuning Hyperparameters Section\n",
        "\n",
        "original optunua notebook at https://github.com/gilfernandes/commonlit/blob/main/72_pytorch_transformers_deberta_optuna.ipynb\n",
        "\n",
        "Optuna documentation: https://optuna.readthedocs.io/en/stable/"
      ],
      "metadata": {
        "id": "e2HGl0ke36IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "kfold = KFold(n_splits=CFG.n_fold, random_state=CFG.seed, shuffle=True)\n",
        "splits = list(kfold.split(train_df))\n",
        "\n",
        "#fold = 0\n",
        "\n",
        "def objective(trial):\n",
        "    epochs = CFG.epochs  #4\n",
        "\n",
        "    #tuning the learning rate\n",
        "    lr = trial.suggest_uniform(\"lr\", 1.25e-5, 2.5e-5)\n",
        "\n",
        "    #tuning lr scheduler hyperparameter\n",
        "    schedule_func = trial.suggest_categorical('schedule_func', [get_cosine_with_hard_restarts_schedule_with_warmup, get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup])\n",
        "\n",
        "    #tuning batch size hyperparameter\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [2, 4, 8])\n",
        "\n",
        "\n",
        "    print(f'##### Using fold {fold}')\n",
        "    print(f'##### Using base_lr {lr}  epochs {epochs}')\n",
        "    print(f'##### Using {schedule_func}')\n",
        "    \n",
        "\n",
        "    model_path = CFG.model\n",
        "\n",
        "    set_random_seed(CFG.seed + fold)\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
        "\n",
        "\n",
        "    print(\"TRAIN var\")\n",
        "    print(train)\n",
        "\n",
        "    \n",
        "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
        "  \n",
        "    optimizer, debug = deberta_base_AdamW_LLRD(model, lr) # for per layer lr\n",
        "    #optimizer, debug = deberta_base_AdamW_grouped_LLRD(model, lr) # for grouped lr\n",
        "    \n",
        "    scheduler = schedule_func(optimizer, num_training_steps=CFG.epochs * len(train), num_warmup_steps=CFG.num_warmup_steps) #num_warmup_steps=50\n",
        "    scaler = torch.cuda.amp.GradScaler() # fp16\n",
        "\n",
        "    trainer, score = train_loop(train, fold, model, optimizer, scheduler, scaler, batch_size) \n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "    \n",
        "    \n",
        "    del model\n",
        "    del tokenizer\n",
        "    del optimizer\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return score #return learning rate also"
      ],
      "metadata": {
        "id": "tGNUNO8_sf5P"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "  for i in CFG.trn_fold:\n",
        "    fold = i\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=CFG.n_trials) #n_trials=20\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "\n",
        "    print(\" Best value: \", study.best_trial.value)\n",
        "    print(\" Best  params: \")\n",
        "    for key, value in study.best_trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "\n",
        "\n",
        "    if CFG.wandb:\n",
        "        wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEhHyFS8sgZZ",
        "outputId": "bcf5ae2c-9c51-4b8e-999c-d736d507a329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-27 23:38:20,459]\u001b[0m A new study created in memory with name: no-name-216d79d9-1f78-442f-b0d2-e0b03ebfc031\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Using fold 0\n",
            "##### Using base_lr 2.3675322291669548e-05  epochs 4\n",
            "##### Using <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7fe026780b90>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN var\n",
            "           text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions  fold\n",
            "0     0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0     1\n",
            "1     0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5     0\n",
            "2     00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5     2\n",
            "3     003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0     2\n",
            "4     0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5     2\n",
            "...            ...                                                ...       ...     ...         ...          ...      ...          ...   ...\n",
            "3906  FFD29828A873  I believe using cellphones in class for educat...       2.5     3.0         3.0          3.5      2.5          2.5     1\n",
            "3907  FFD9A83B0849  Working alone, students do not have to argue w...       4.0     4.0         4.0          4.0      3.5          3.0     1\n",
            "3908  FFDC4011AC9C  \"A problem is a chance for you to do your best...       2.5     3.0         3.0          3.0      3.5          3.0     3\n",
            "3909  FFE16D704B16  Many people disagree with Albert Schweitzer's ...       4.0     4.5         4.5          4.0      4.5          4.5     2\n",
            "3910  FFED00D6E0BD  Do you think that failure is the main thing fo...       3.5     2.5         3.5          3.0      3.0          3.5     2\n",
            "\n",
            "[3911 rows x 9 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "INFO:__main__:DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.22.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 128100\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "========== fold: 0 training ==========\n",
            "INFO:__main__:========== fold: 0 training ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_FN()\n",
            "Epoch: [1][0/366] Elapsed 0m 3s (remain 22m 43s) Loss: 0.0488(0.0488) Grad: 39553.3906  LR: 0.00002368  \n",
            "Epoch: [1][20/366] Elapsed 0m 43s (remain 11m 55s) Loss: 0.0265(0.0373) Grad: 19499.0000  LR: 0.00002368  \n",
            "Epoch: [1][40/366] Elapsed 1m 15s (remain 10m 0s) Loss: 0.0172(0.0301) Grad: 17845.4922  LR: 0.00002367  \n",
            "Epoch: [1][60/366] Elapsed 1m 59s (remain 9m 57s) Loss: 0.0068(0.0240) Grad: 11639.9180  LR: 0.00002367  \n",
            "Epoch: [1][80/366] Elapsed 2m 35s (remain 9m 6s) Loss: 0.0017(0.0187) Grad: 3720.3997  LR: 0.00002367  \n",
            "Epoch: [1][100/366] Elapsed 3m 6s (remain 8m 8s) Loss: 0.0016(0.0154) Grad: 11655.4375  LR: 0.00002367  \n",
            "Epoch: [1][120/366] Elapsed 3m 39s (remain 7m 24s) Loss: 0.0010(0.0131) Grad: 2115.2310  LR: 0.00002367  \n",
            "Epoch: [1][140/366] Elapsed 4m 15s (remain 6m 47s) Loss: 0.0010(0.0114) Grad: 2006.0400  LR: 0.00002367  \n",
            "Epoch: [1][160/366] Elapsed 4m 55s (remain 6m 15s) Loss: 0.0013(0.0102) Grad: 4521.8174  LR: 0.00002367  \n",
            "Epoch: [1][180/366] Elapsed 5m 34s (remain 5m 42s) Loss: 0.0013(0.0092) Grad: 3795.6462  LR: 0.00002367  \n",
            "Epoch: [1][200/366] Elapsed 6m 8s (remain 5m 2s) Loss: 0.0011(0.0084) Grad: 4074.3411  LR: 0.00002367  \n",
            "Epoch: [1][220/366] Elapsed 6m 43s (remain 4m 24s) Loss: 0.0015(0.0078) Grad: 2020.9633  LR: 0.00002366  \n",
            "Epoch: [1][240/366] Elapsed 7m 17s (remain 3m 46s) Loss: 0.0012(0.0072) Grad: 2649.3733  LR: 0.00002366  \n",
            "Epoch: [1][260/366] Elapsed 7m 51s (remain 3m 9s) Loss: 0.0015(0.0068) Grad: 4645.7461  LR: 0.00002366  \n",
            "Epoch: [1][280/366] Elapsed 8m 28s (remain 2m 33s) Loss: 0.0010(0.0064) Grad: 1498.9667  LR: 0.00002366  \n",
            "Epoch: [1][300/366] Elapsed 9m 7s (remain 1m 58s) Loss: 0.0010(0.0060) Grad: 1746.3125  LR: 0.00002365  \n",
            "Epoch: [1][320/366] Elapsed 9m 38s (remain 1m 21s) Loss: 0.0010(0.0057) Grad: 1640.7438  LR: 0.00002365  \n",
            "Epoch: [1][340/366] Elapsed 10m 12s (remain 0m 44s) Loss: 0.0013(0.0054) Grad: 4874.1875  LR: 0.00002365  \n",
            "Epoch: [1][360/366] Elapsed 10m 48s (remain 0m 8s) Loss: 0.0010(0.0052) Grad: 3190.1421  LR: 0.00002364  \n",
            "Epoch: [1][365/366] Elapsed 10m 56s (remain 0m 0s) Loss: 0.0009(0.0052) Grad: 2830.8088  LR: 0.00002364  \n",
            "valid_FN()\n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 26s) Loss: 0.0013(0.0013) \n",
            "EVAL: [20/62] Elapsed 0m 25s (remain 0m 50s) Loss: 0.0013(0.0012) \n",
            "EVAL: [40/62] Elapsed 0m 47s (remain 0m 24s) Loss: 0.0009(0.0011) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - avg_train_loss: 0.0052  avg_val_loss: 0.0011  time: 728s\n",
            "INFO:__main__:Epoch 1 - avg_train_loss: 0.0052  avg_val_loss: 0.0011  time: 728s\n",
            "Epoch 1 - Score: 0.4734  Scores: [0.5163106398093098, 0.4680796682267831, 0.44271664692435414, 0.4701677909903596, 0.4820547937669303, 0.46084543971116665]\n",
            "INFO:__main__:Epoch 1 - Score: 0.4734  Scores: [0.5163106398093098, 0.4680796682267831, 0.44271664692435414, 0.4701677909903596, 0.4820547937669303, 0.46084543971116665]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL: [60/62] Elapsed 1m 10s (remain 0m 1s) Loss: 0.0008(0.0011) \n",
            "EVAL: [61/62] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0005(0.0011) \n",
            "Score\n",
            "0.47336249657148394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Save Best Score: 0.4734 Model\n",
            "INFO:__main__:Epoch 1 - Save Best Score: 0.4734 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_FN()\n",
            "Epoch: [2][0/366] Elapsed 0m 1s (remain 8m 43s) Loss: 0.0012(0.0012) Grad: 4925.2319  LR: 0.00002364  \n",
            "Epoch: [2][20/366] Elapsed 0m 38s (remain 10m 40s) Loss: 0.0015(0.0012) Grad: 5591.6602  LR: 0.00002364  \n",
            "Epoch: [2][40/366] Elapsed 1m 14s (remain 9m 50s) Loss: 0.0013(0.0012) Grad: 2851.9287  LR: 0.00002364  \n",
            "Epoch: [2][60/366] Elapsed 1m 45s (remain 8m 45s) Loss: 0.0015(0.0011) Grad: 2961.1160  LR: 0.00002363  \n",
            "Epoch: [2][80/366] Elapsed 2m 24s (remain 8m 27s) Loss: 0.0013(0.0011) Grad: 4368.7417  LR: 0.00002363  \n",
            "Epoch: [2][100/366] Elapsed 3m 3s (remain 8m 0s) Loss: 0.0006(0.0011) Grad: 2463.0508  LR: 0.00002362  \n",
            "Epoch: [2][120/366] Elapsed 3m 46s (remain 7m 38s) Loss: 0.0012(0.0011) Grad: 2190.1479  LR: 0.00002362  \n",
            "Epoch: [2][140/366] Elapsed 4m 20s (remain 6m 55s) Loss: 0.0009(0.0011) Grad: 2316.3127  LR: 0.00002361  \n",
            "Epoch: [2][160/366] Elapsed 4m 55s (remain 6m 16s) Loss: 0.0007(0.0011) Grad: 3009.5540  LR: 0.00002361  \n",
            "Epoch: [2][180/366] Elapsed 5m 34s (remain 5m 41s) Loss: 0.0009(0.0011) Grad: 2385.2676  LR: 0.00002360  \n",
            "Epoch: [2][200/366] Elapsed 6m 6s (remain 5m 0s) Loss: 0.0012(0.0011) Grad: 3224.6694  LR: 0.00002360  \n",
            "Epoch: [2][220/366] Elapsed 6m 40s (remain 4m 22s) Loss: 0.0014(0.0011) Grad: 3446.1643  LR: 0.00002359  \n",
            "Epoch: [2][240/366] Elapsed 7m 21s (remain 3m 48s) Loss: 0.0007(0.0011) Grad: 2120.3723  LR: 0.00002359  \n",
            "Epoch: [2][260/366] Elapsed 7m 51s (remain 3m 9s) Loss: 0.0008(0.0011) Grad: 1838.0576  LR: 0.00002358  \n",
            "Epoch: [2][280/366] Elapsed 8m 27s (remain 2m 33s) Loss: 0.0009(0.0011) Grad: 1802.8569  LR: 0.00002358  \n",
            "Epoch: [2][300/366] Elapsed 9m 1s (remain 1m 57s) Loss: 0.0007(0.0011) Grad: 2794.8684  LR: 0.00002357  \n",
            "Epoch: [2][320/366] Elapsed 9m 39s (remain 1m 21s) Loss: 0.0018(0.0011) Grad: 2062.9727  LR: 0.00002356  \n",
            "Epoch: [2][340/366] Elapsed 10m 12s (remain 0m 44s) Loss: 0.0014(0.0011) Grad: 3137.5667  LR: 0.00002356  \n",
            "Epoch: [2][360/366] Elapsed 10m 41s (remain 0m 8s) Loss: 0.0012(0.0011) Grad: 2740.1389  LR: 0.00002355  \n",
            "Epoch: [2][365/366] Elapsed 10m 52s (remain 0m 0s) Loss: 0.0008(0.0011) Grad: 2093.8010  LR: 0.00002355  \n",
            "valid_FN()\n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 17s) Loss: 0.0012(0.0012) \n",
            "EVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.0011(0.0012) \n",
            "EVAL: [40/62] Elapsed 0m 46s (remain 0m 23s) Loss: 0.0011(0.0012) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0009(0.0012) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0005(0.0012) \n",
            "Score\n",
            "0.49032968428767787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - avg_train_loss: 0.0011  avg_val_loss: 0.0012  time: 722s\n",
            "INFO:__main__:Epoch 2 - avg_train_loss: 0.0011  avg_val_loss: 0.0012  time: 722s\n",
            "Epoch 2 - Score: 0.4903  Scores: [0.5455797751686231, 0.4707641013265915, 0.4534472460595811, 0.5012616993944063, 0.49912365833567623, 0.4718016254411888]\n",
            "INFO:__main__:Epoch 2 - Score: 0.4903  Scores: [0.5455797751686231, 0.4707641013265915, 0.4534472460595811, 0.5012616993944063, 0.49912365833567623, 0.4718016254411888]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_FN()\n",
            "Epoch: [3][0/366] Elapsed 0m 1s (remain 9m 54s) Loss: 0.0012(0.0012) Grad: 2365.4771  LR: 0.00002355  \n",
            "Epoch: [3][20/366] Elapsed 0m 34s (remain 9m 30s) Loss: 0.0010(0.0010) Grad: 4277.0137  LR: 0.00002354  \n",
            "Epoch: [3][40/366] Elapsed 1m 4s (remain 8m 35s) Loss: 0.0015(0.0010) Grad: 4845.6592  LR: 0.00002353  \n",
            "Epoch: [3][60/366] Elapsed 1m 42s (remain 8m 34s) Loss: 0.0009(0.0010) Grad: 2266.6514  LR: 0.00002353  \n",
            "Epoch: [3][80/366] Elapsed 2m 18s (remain 8m 9s) Loss: 0.0008(0.0010) Grad: 1242.3086  LR: 0.00002352  \n",
            "Epoch: [3][100/366] Elapsed 2m 59s (remain 7m 50s) Loss: 0.0012(0.0010) Grad: 3055.2129  LR: 0.00002351  \n",
            "Epoch: [3][120/366] Elapsed 3m 32s (remain 7m 10s) Loss: 0.0011(0.0010) Grad: 2738.7505  LR: 0.00002350  \n",
            "Epoch: [3][140/366] Elapsed 4m 7s (remain 6m 34s) Loss: 0.0008(0.0010) Grad: 1904.3298  LR: 0.00002349  \n",
            "Epoch: [3][160/366] Elapsed 4m 40s (remain 5m 57s) Loss: 0.0006(0.0010) Grad: 2406.8542  LR: 0.00002349  \n",
            "Epoch: [3][180/366] Elapsed 5m 17s (remain 5m 24s) Loss: 0.0006(0.0010) Grad: 1796.7578  LR: 0.00002348  \n",
            "Epoch: [3][200/366] Elapsed 5m 55s (remain 4m 51s) Loss: 0.0008(0.0010) Grad: 2408.9409  LR: 0.00002347  \n",
            "Epoch: [3][220/366] Elapsed 6m 26s (remain 4m 13s) Loss: 0.0008(0.0010) Grad: 2300.3799  LR: 0.00002346  \n",
            "Epoch: [3][240/366] Elapsed 7m 8s (remain 3m 42s) Loss: 0.0009(0.0010) Grad: 2384.8357  LR: 0.00002345  \n",
            "Epoch: [3][260/366] Elapsed 7m 41s (remain 3m 5s) Loss: 0.0009(0.0010) Grad: 1599.9233  LR: 0.00002344  \n",
            "Epoch: [3][280/366] Elapsed 8m 14s (remain 2m 29s) Loss: 0.0011(0.0010) Grad: 2458.1172  LR: 0.00002343  \n",
            "Epoch: [3][300/366] Elapsed 8m 48s (remain 1m 54s) Loss: 0.0008(0.0010) Grad: 2072.6125  LR: 0.00002342  \n",
            "Epoch: [3][320/366] Elapsed 9m 24s (remain 1m 19s) Loss: 0.0007(0.0010) Grad: 2427.4963  LR: 0.00002341  \n",
            "Epoch: [3][340/366] Elapsed 9m 53s (remain 0m 43s) Loss: 0.0005(0.0010) Grad: 1491.4108  LR: 0.00002340  \n",
            "Epoch: [3][360/366] Elapsed 10m 28s (remain 0m 8s) Loss: 0.0006(0.0010) Grad: 1749.2965  LR: 0.00002339  \n",
            "Epoch: [3][365/366] Elapsed 10m 38s (remain 0m 0s) Loss: 0.0013(0.0010) Grad: 4846.8325  LR: 0.00002339  \n",
            "valid_FN()\n",
            "EVAL: [0/62] Elapsed 0m 2s (remain 2m 18s) Loss: 0.0012(0.0012) \n",
            "EVAL: [20/62] Elapsed 0m 25s (remain 0m 49s) Loss: 0.0013(0.0011) \n",
            "EVAL: [40/62] Elapsed 0m 46s (remain 0m 23s) Loss: 0.0011(0.0011) \n",
            "EVAL: [60/62] Elapsed 1m 9s (remain 0m 1s) Loss: 0.0008(0.0011) \n",
            "EVAL: [61/62] Elapsed 1m 9s (remain 0m 0s) Loss: 0.0004(0.0011) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - avg_train_loss: 0.0010  avg_val_loss: 0.0011  time: 708s\n",
            "INFO:__main__:Epoch 3 - avg_train_loss: 0.0010  avg_val_loss: 0.0011  time: 708s\n",
            "Epoch 3 - Score: 0.4666  Scores: [0.5129385304703922, 0.4579645969993048, 0.4339559315156671, 0.45669712065242846, 0.4749089648809594, 0.4628837333418272]\n",
            "INFO:__main__:Epoch 3 - Score: 0.4666  Scores: [0.5129385304703922, 0.4579645969993048, 0.4339559315156671, 0.45669712065242846, 0.4749089648809594, 0.4628837333418272]\n",
            "Epoch 3 - Save Best Score: 0.4666 Model\n",
            "INFO:__main__:Epoch 3 - Save Best Score: 0.4666 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score\n",
            "0.4665581463100965\n",
            "TRAIN_FN()\n",
            "Epoch: [4][0/366] Elapsed 0m 1s (remain 9m 59s) Loss: 0.0011(0.0011) Grad: 2551.1313  LR: 0.00002339  \n",
            "Epoch: [4][20/366] Elapsed 0m 39s (remain 10m 49s) Loss: 0.0014(0.0010) Grad: 3252.7998  LR: 0.00002338  \n",
            "Epoch: [4][40/366] Elapsed 1m 9s (remain 9m 12s) Loss: 0.0008(0.0010) Grad: 1373.0078  LR: 0.00002337  \n",
            "Epoch: [4][60/366] Elapsed 1m 48s (remain 9m 1s) Loss: 0.0011(0.0010) Grad: 4162.6138  LR: 0.00002336  \n",
            "Epoch: [4][80/366] Elapsed 2m 25s (remain 8m 32s) Loss: 0.0009(0.0010) Grad: 3124.8884  LR: 0.00002335  \n",
            "Epoch: [4][100/366] Elapsed 3m 4s (remain 8m 2s) Loss: 0.0008(0.0010) Grad: 3123.9592  LR: 0.00002333  \n",
            "Epoch: [4][120/366] Elapsed 3m 42s (remain 7m 29s) Loss: 0.0008(0.0010) Grad: 2523.2688  LR: 0.00002332  \n",
            "Epoch: [4][140/366] Elapsed 4m 13s (remain 6m 44s) Loss: 0.0012(0.0010) Grad: 2192.7014  LR: 0.00002331  \n",
            "Epoch: [4][160/366] Elapsed 4m 53s (remain 6m 13s) Loss: 0.0011(0.0010) Grad: 2945.4778  LR: 0.00002330  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "output upon completion:\n",
        "\n",
        "[I 2022-09-27 23:26:23,723] Trial 1 finished with value: 0.4850880549344474 and parameters: {'lr': 1.6550251604962863e-05, 'schedule_func': <function get_cosine_with_hard_restarts_schedule_with_warmup at 0x7fb83ad19a70>, 'batch_size': 8}. Best is trial 0 with value: 0.4750748158409344.\n",
        "Study statistics: \n",
        "  Number of finished trials:  2\n",
        "  Number of pruned trials:  0\n",
        "  Number of complete trials:  2\n",
        "Best trial:\n",
        "  Value:  0.4750748158409344\n",
        "  Params: \n",
        "    lr: 1.321538910821276e-05\n",
        "    schedule_func: <function get_polynomial_decay_schedule_with_warmup at 0x7fb83ad19b00>\n",
        "    batch_size: 2\n",
        " Best value:  0.4750748158409344\n",
        " Best  params: \n",
        "    lr: 1.321538910821276e-05\n",
        "    schedule_func: <function get_polynomial_decay_schedule_with_warmup at 0x7fb83ad19b00>\n",
        "    batch_size: 2\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_gbI-aThsg0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "03spSK6Vpzqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#./microsoft-deberta-v3-base_fold2_best.pth"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-27T05:56:18.554449Z",
          "iopub.execute_input": "2022-09-27T05:56:18.555059Z",
          "iopub.status.idle": "2022-09-27T05:56:18.560694Z",
          "shell.execute_reply.started": "2022-09-27T05:56:18.555021Z",
          "shell.execute_reply": "2022-09-27T05:56:18.559758Z"
        },
        "trusted": true,
        "id": "Zs2ggX8-pzqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
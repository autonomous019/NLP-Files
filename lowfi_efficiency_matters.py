# -*- coding: utf-8 -*-
"""lowfi-efficiency-matters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w8fgthF6jmurS-o6zFqlD8gJnFKlJv-6
"""

import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoConfig, AutoModel
import os
from types import SimpleNamespace
import yaml
import multiprocessing as mp
from glob import glob
from torch import nn
from torch.utils.data import Dataset, DataLoader
import collections
import lightgbm
     
os.environ["TOKENIZERS_PARALLELISM"] = "false"

#this work is based on https://www.kaggle.com/code/philippsinger/team-hydrogen-efficiency-prize-1st-place
#data for original version https://www.kaggle.com/code/ryanholbrook/feedback-prize-efficiency-leaderboard/data


#built for "baby, ain't got no GPU, damn!"


# ====================================================
# Directory settings
# ====================================================
import os

OUTPUT_DIR = './'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)



# ====================================================
# CFG
# ====================================================
EXP_NAME = "feedback-prize-english-language-learning"

class CFG:
    my_device=device
    competition='FB3'
    debug=False
    num_workers=4
    model="microsoft/deberta-v3-base"
    regression_lr=1e-6
    max_len=512 #originally 512
    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
    seed=1916
    #cache_dir = f"../input/{EXP_NAME}/deberta-v3-base/"
    cache_dir = f"../input/debertav3base-models/deberta-v3-base/"  
    _tokenizer_sep_token = ""
    _tokenizer_start_token_id = []
    _tokenizer_end_token_id = []
    _tokenizer_size = ""

    
#if CFG.debug:
    #some debug settings here

# ====================================================
# Directory settings
# ====================================================
import os
# Import from GoogleDrive
from google.colab import drive
drive.mount('/content/gdrive')
os.chdir("//content/gdrive/MyDrive/feedback-prize-english-language-learning")

save_dir = "/content/gdrive/My Drive/feedback-prize-english-language-learning/submission"
logs_dir = "/content/gdrive/My Drive/feedback-prize-english-language-learning/logs"
data_dir = "/content/gdrive/My Drive/feedback-prize-english-language-learning"
model_dir = "/content/gdrive/My Drive/ml_models/model"
import os.path
from os import path

model_folder = CFG.model.replace('/', '-')
if path.exists(data_dir + "/" + "trained_" + model_folder) == False:
  os.mkdir(data_dir + "/" + "trained_" + model_folder)
save_model_dir = data_dir + "/" + "trained_" + model_folder

CFG.cache_dir = save_model_dir

N_CORES = mp.cpu_count()

if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):
    data_folder = "test"
    df = pd.read_csv(data_dir + "/test.csv")
    CALC_SCORE = False
    print("testing")
    print(df.head())
else:
    data_folder = "train"
    df = pd.read_csv(data_dir + "/train.csv")
    ids = df.text_id.unique()
    np.random.seed(CFG.seed)
    val_ids = np.random.choice(ids, size=3000, replace=False)
    df = df[df.text_id.isin(val_ids)]
    df = df.reset_index(drop=True)
    CALC_SCORE = True
    print("training")
    print(df.head())

full_texts = df.loc[:,["text_id","full_text"]]
print(full_texts)

#tokenizer = AutoTokenizer.from_pretrained(CFG.model)
tokenizer = AutoTokenizer.from_pretrained(CFG.cache_dir)

CFG._tokenizer_sep_token = tokenizer.sep_token

CFG._tokenizer_start_token_id = []
CFG._tokenizer_end_token_id = []

d_types = sorted(df.syntax.unique()) #was discourse types

for t in d_types:
    tokenizer.add_tokens([f"[START_{t}]"], special_tokens=True)
    CFG._tokenizer_start_token_id.append(tokenizer.encode(f"[START_{t}]")[1])
    
for t in d_types:
    tokenizer.add_tokens([f"[END_{t}]"], special_tokens=True)
    CFG._tokenizer_end_token_id.append(tokenizer.encode(f"[END_{t}]")[1])

tokenizer.add_tokens([f"\n"], special_tokens=True)
CFG._tokenizer_size = len(tokenizer)

grps = df.groupby("text_id", sort=False)
grp_texts = []

for grp in grps.groups:
    g = grps.get_group(grp)
    t = g.full_text.values[0]

    end = 0
    for j in range(len(g)):
        d = g.full_text.values[j]
        start = t[end:].find(d.strip())
        start = start + end

        end = start + len(d.strip())
        t = (
            t[:start]
            + f" [START_{g.full_text.values[j]}]  "
            + t[start:end]
            + f" [END_{g.full_text.values[j]}] "
            + t[end:]
        )

    t = " ".join(g.full_text.values) + f" {CFG._tokenizer_sep_token} " + t
    grp_texts.append(t)

def encode(text):
    sample = dict()
    encodings = tokenizer(
        text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        #max_length=cfg.tokenizer.max_length,
        max_length=CFG.max_len,
    )
    sample["input_ids"] = encodings["input_ids"][0]
    sample["attention_mask"] = encodings["attention_mask"][0]
    return sample

pool_obj = mp.Pool(N_CORES)
grp_texts = pool_obj.map(encode, grp_texts)

lens = [torch.sum(x["attention_mask"]).item() for x in grp_texts]

lens_map = df[["text_id"]].drop_duplicates()
lens_map["count"] = lens
lens_map["orig_text_order"] = range(len(lens_map))

df["orig_order"] = range(len(df))
df = df.merge(lens_map)
df = df.sort_values(["count", "text_id", "orig_order"], ascending=True).reset_index(drop=True)
grp_texts = [grp_texts[i] for i in df["orig_text_order"].unique()]

class FeedbackDataset(Dataset):
    def __init__(self, grp_texts):
        self.grp_texts = grp_texts

    def __len__(self):
        return len(self.grp_texts)

    def batch_to_device(batch, device):
        if isinstance(batch, torch.Tensor):
            return batch.to(device)
        elif isinstance(batch, collections.abc.Mapping):
            return {
                key: FeedbackDataset.batch_to_device(value, device)
                for key, value in batch.items()
            }

    def __getitem__(self, idx):
        sample = self.grp_texts[idx]
        if idx == 0:
            print(sample)
        return sample

class NLPAllclsTokenPooling(nn.Module):
    def __init__(self, dim):
        super(NLPAllclsTokenPooling, self).__init__()

        self.dim = dim
        self.feat_mult = 3

    def forward(self, x, attention_mask, input_ids, cfg):
        ret = []
        for j in range(x.shape[0]):
            idx0 = torch.where(
                (input_ids[j] >= min(CFG._tokenizer_start_token_id))
                & (input_ids[j] <= max(CFG._tokenizer_start_token_id))
            )[0]
            idx1 = torch.where(
                (input_ids[j] >= min(CFG._tokenizer_end_token_id))
                & (input_ids[j] <= max(CFG._tokenizer_end_token_id))
            )[0]

            xx = []
            for jj in range(len(idx0)):
                xx0 = x[j, idx0[jj]]
                xx1 = x[j, idx1[jj]]
                xx2 = x[j, idx0[jj] + 1 : idx1[jj]].mean(dim=0)
                xxx = torch.cat([xx0, xx1, xx2]).unsqueeze(0)
                xx.append(xxx)
            xx = torch.cat(xx)
            ret.append(xx)

        return ret


class FeedbackModel(nn.Module):
    def __init__(self, cfg):
        super(FeedbackModel, self).__init__()

        self.cfg = cfg
        self.n_classes = 3
        #config = AutoConfig.from_pretrained(CFG.model)
        config = AutoConfig.from_pretrained(CFG.cache_dir)
        self.backbone = AutoModel.from_config(config)
        self.backbone.pooler = None
        self.backbone.resize_token_embeddings(CFG._tokenizer_size)

        self.pooling = NLPAllclsTokenPooling(
            dim=1
        )  # init pooling and pool over token dimension
        self.head = nn.Linear(
            self.backbone.config.hidden_size * self.pooling.feat_mult, self.n_classes
        )

    def get_features(self, batch):
        attention_mask = batch["attention_mask"]
        input_ids = batch["input_ids"]

        x = self.backbone(
            input_ids=input_ids, attention_mask=attention_mask
        ).last_hidden_state

        x = self.pooling(x, attention_mask, input_ids, cfg=self.cfg)
        x = torch.cat(x)

        return x

    def forward(self, batch):
        idx = int(torch.where(batch["attention_mask"] == 1)[1].max())
        idx += 1
        batch["attention_mask"] = batch["attention_mask"][:, :idx]
        batch["input_ids"] = batch["input_ids"][:, :idx]

        x = self.get_features(batch)
        logits = self.head(x)

        return {"logits": logits}

def run_predictions(cfg, grp_texts, bs=1):
    ds = FeedbackDataset(grp_texts)
    device = CFG.my_device
    print(device)
    model = FeedbackModel(CFG.cache_dir).to(device).eval()
    d = torch.load(f"../input/checkpoints/microsoft-deberta-v3-base_fold2_best.pth", map_location=device)
    
    model.load_state_dict(collections.OrderedDict(d["model"]), strict=False)

    dl = DataLoader(ds, shuffle=False, batch_size=bs, num_workers=N_CORES)
    #print(dl.size())
    
    with torch.inference_mode():
        preds = []
        for batch in dl:
            batch = FeedbackDataset.batch_to_device(batch, device)
            
            if device == "cuda":
              
                with torch.cuda.amp.autocast():
                  out = model(batch)
                  preds.append(
                      out["logits"].float().softmax(dim=1).detach().cpu().numpy()
                  )
            else:
              
                with torch.cpu.amp.autocast():
                  out = model(batch)
                  preds.append(
                      out["logits"].float().softmax(dim=1).detach().cpu().numpy()
                  )

    return np.concatenate(preds, axis=0)

#from huggingface_hub import notebook_login
#notebook_login() #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

#pp = run_predictions(cfg, grp_texts=grp_texts, bs=16)

print(CFG.my_device)
pp = run_predictions(CFG, grp_texts=grp_texts, bs=16)

# target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
df["cohesion"] = pp[:, 0] 
df["syntax"] = pp[:, 1] 
df["vocabulary"] = pp[:, 2]
df["phraseology"] = pp[:,3]
df["grammar"] = pp[:,4]
df["conventions"] = pp[:,5]

orig_preds = pp.copy()

#target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
label_cols = CFG.target_cols
oof_cols = []
for j, l in enumerate(label_cols):

    df[f"oof_{l}"] = pp[:,j]
    oof_cols.append(f"oof_{l}")

class FeedbackStackerModel(nn.Module):
    def __init__(self, n_features):
        super(FeedbackStackerModel, self).__init__()

        self.sizes = [256, 128, 64]

        self.features = nn.Sequential(
            nn.utils.weight_norm(nn.Linear(n_features, self.sizes[0])),
            nn.PReLU(),
            nn.Linear(self.sizes[0], self.sizes[1]),
            nn.PReLU(),
            nn.Linear(self.sizes[1], self.sizes[2]),
            nn.PReLU(),
        )
        self.head = nn.Linear(self.sizes[-1], 3)

        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, x, y):
        x = self.features(x)
        x = self.head(x)

        return {"logits": x}


class FeedbackStackerDataset(Dataset):
    def __init__(self, df, mode):
        self.df = df.copy().reset_index(drop=True)
        self.mode = mode

        self.feature_cols = oof_cols.copy()
        self.label_cols = label_cols.copy()

        df = self.df
        '''
        df["len"] = df.groupby("text_id")[f"discourse_id"].transform("count") / 10
        self.feature_cols.append("len")
        '''
        df["len"] = df.groupby("text_id").transform("count") / 10
        self.feature_cols.append("len")

        for j, l in enumerate(label_cols):
            df[f"oof_{l}_mean"] = df.groupby("text_id")[f"oof_{l}"].transform("mean")
            self.feature_cols.append(f"oof_{l}_mean")
            df[f"oof_{l}_t_mean"] = df.groupby(["text_id"])[
                f"oof_{l}"
            ].transform("mean")
            
            '''
            df[f"oof_{l}_t_mean"] = df.groupby(["text_id", "discourse_type"])[
                f"oof_{l}"
            ].transform("mean")
            '''
            self.feature_cols.append(f"oof_{l}_t_mean")

        self.num_features = len(self.feature_cols)

        self.X = self.df[self.feature_cols].values
        self.y = self.df[self.label_cols].values

    def __getitem__(self, idx):
        X = self.X[idx]
        y = self.y[idx]

        return torch.FloatTensor(X), torch.FloatTensor(y)

    def __len__(self):
        return self.df.shape[0]


ds = FeedbackStackerDataset(df.copy(), mode="val")


def run_nn_stacker(df, BS=64):
    ds = FeedbackStackerDataset(df.iloc[:].copy(), mode="test")
    #../input/checkpoints/microsoft-deberta-v3-base_fold2_best.pth
    checkpoints = glob(f"../input/checkpoints/*.pth")
    device = CFG.my_device

    models = []
    for checkpoint in checkpoints:
        print(f"running model {checkpoint}")

        model = FeedbackStackerModel(n_features=ds.num_features).to(device).eval()
        model_weights = torch.load(checkpoint, map_location=device)

        model.load_state_dict(collections.OrderedDict(model_weights), strict=True)
        models.append(model)

    dl = DataLoader(ds, shuffle=False, batch_size=BS, num_workers=N_CORES)

    with torch.no_grad():
        preds = []
        for batch in dl:
            data = [x.to("cuda") for x in batch]
            inputs, target = data
            p = []
            for model in models:
                out = model(inputs, target)
                p.append(out["logits"].float().softmax(dim=1))
            preds.append(torch.mean(torch.stack(p), dim=0).detach().cpu().numpy())

    preds = np.concatenate(preds, axis=0)

    return preds


nn_stacker_preds = run_nn_stacker(df, BS=256)

def gen_x(values):
    return np.histogram(
        np.clip(values, 0.001, 0.999), bins=3, density=True, range=(0, 1)
    )[0]

'''
all_groups = []

gb = df.groupby("text_id", sort=False)

df["n_types"] = gb["syntax"].transform(lambda x: x.nunique())
df["mean_Ineffective"] = gb["Ineffective"].transform("mean")

for name, group in gb:
    class_name = "Ineffective"
    for idx, val in enumerate(gen_x(group[class_name].values)):
        group[f"{class_name}_bin_{idx}"] = val

    all_groups.append(group)

df = pd.concat(all_groups).reset_index(drop=True)

disc_types_mapping = {
    "Lead": 0,
    "Position": 1,
    "Claim": 2,
    "Evidence": 3,
    "Counterclaim": 4,
    "Rebuttal": 5,
    "Concluding Statement": 6,
}

df["len_disc"] = df.discourse_text.str.len()
df["discourse_type"] = df["discourse_type"].map(disc_types_mapping)
df["paragraph_cnt"] = df.essay_text.map(lambda x: len(x.split("\n\n")))
'''

#use xgboost instead

lgb_stacker_preds = []

for fold in range(5):
    gbm = lightgbm.Booster(model_file=f"../input/efficiency-prize-v2/lightgbm/model_fold_{fold}.txt")
    valid_pred = gbm.predict(
        df[
            [
                "discourse_type",
                "Adequate",
                "Effective",
                "Ineffective",
                "n_types",
                "Ineffective_bin_0",
                "Ineffective_bin_1",
                "Ineffective_bin_2",
                "mean_Ineffective",
                "len_disc",
                "paragraph_cnt",
            ]
        ]
    )
    lgb_stacker_preds.append(valid_pred)
    
lgb_stacker_preds = np.array(lgb_stacker_preds).mean(axis=0)

all_preds = [
    orig_preds,
    lgb_stacker_preds,
    nn_stacker_preds,
]

all_preds = np.average(all_preds, axis=0, weights=[2, 1, 1 ])

df["Adequate"] = all_preds[:, 0]
df["Effective"] = all_preds[:, 1]
df["Ineffective"] = all_preds[:, 2]

df[["discourse_id", "Ineffective", "Adequate", "Effective"]].to_csv("submission.csv", index=False)

if CALC_SCORE:
    from sklearn.metrics import log_loss
    
    label_cols = ["Adequate", "Effective", "Ineffective"]
    
    y = np.zeros_like(all_preds)
    
    for ii, jj in enumerate([label_cols.index(x) for x in df["discourse_effectiveness"].values]):
        y[ii,jj] = 1
        
    print(log_loss(y, df[label_cols]))

